{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b7ba7b-433c-463c-8e5e-8b975a5be463",
   "metadata": {},
   "source": [
    "# Machine Learning In Production\n",
    "\n",
    "This notebook was created by [Santiago L. Valdarrama](https://twitter.com/svpino) as part of the [Machine Learning School](https://www.ml.school) program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b80dfa35-b345-4f38-8426-f0223de0c401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72eb4984-3c21-4033-be45-2de2cfc1257c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.22.22 requires botocore==1.23.22, but you have botocore 1.29.111 which is incompatible.\n",
      "awscli 1.22.22 requires s3transfer<0.6.0,>=0.5.0, but you have s3transfer 0.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mName: sagemaker\n",
      "Version: 2.145.0\n",
      "Summary: Open source library for training and deploying models on Amazon SageMaker.\n",
      "Home-page: https://github.com/aws/sagemaker-python-sdk/\n",
      "Author: Amazon Web Services\n",
      "Author-email: \n",
      "License: Apache License 2.0\n",
      "Location: /usr/local/lib/python3.8/site-packages\n",
      "Requires: attrs, boto3, google-pasta, importlib-metadata, jsonschema, numpy, packaging, pandas, pathos, platformdirs, protobuf, protobuf3-to-dict, PyYAML, schema, smdebug-rulesconfig\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# Let's make sure we are running the latest version of the SakeMaker's SDK. \n",
    "# Restart the notebook after you upgrade the library.\n",
    "\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q --upgrade sagemaker\n",
    "!pip show sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992fc69-4868-4e50-a50e-76da237f70ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Session 1 - Getting Started\n",
    "\n",
    "This session aims to build a simple [SageMaker Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) with one step to preprocess the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "915b1d0b-d9da-4529-aca5-fd1c08a36f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import argparse\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString, ParameterFloat\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb24b35-450a-4b96-a544-38f084433b62",
   "metadata": {},
   "source": [
    "## Step 1 - Creating an S3 Bucket\n",
    "\n",
    "We need to create an S3 bucket where we will upload everything we need during the program.\n",
    "\n",
    "Make sure you set `BUCKET` to the name of the bucket you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea8d2fd-7dd4-43c8-b9d9-5372448d72a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Location\": \"/mlschool\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "BUCKET = \"mlschool\"\n",
    "\n",
    "!aws s3api create-bucket --bucket $BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accefa0d-31d6-4df4-814a-d74c703689b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2 - Downloading the Dataset\n",
    "\n",
    "We can now download the [Penguins dataset](https://www.kaggle.com/parulpandey/palmer-archipelago-antarctica-penguin-data) and store it in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "882a8be9-dc31-4df1-9009-3b2d0284cb70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset S3 location: s3://mlschool/penguins/data.csv\n"
     ]
    }
   ],
   "source": [
    "S3_FILEPATH = f\"s3://{BUCKET}/penguins\"\n",
    "DATA_FILEPATH = \"penguins/data.csv\"\n",
    "\n",
    "# Download the official Penguins dataset and store it locally.\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins_size.csv\", \n",
    "    DATA_FILEPATH\n",
    ")\n",
    "\n",
    "# Upload the dataset to S3. We need to do this to make it available to \n",
    "# the preprocessing step.\n",
    "INPUT_DATA_URI = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=DATA_FILEPATH, \n",
    "    desired_s3_uri=S3_FILEPATH,\n",
    ")\n",
    "\n",
    "print(f\"Dataset S3 location: {INPUT_DATA_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1efbc-5f66-4b44-9555-68a4731e8e7b",
   "metadata": {},
   "source": [
    "We can now load and display the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a19b546a-6884-483b-8cda-0521e2656a99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>46.8</td>\n",
       "      <td>14.3</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4850.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>50.4</td>\n",
       "      <td>15.7</td>\n",
       "      <td>222.0</td>\n",
       "      <td>5750.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>45.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>212.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>49.9</td>\n",
       "      <td>16.1</td>\n",
       "      <td>213.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    species     island  culmen_length_mm  culmen_depth_mm  flipper_length_mm  \\\n",
       "0    Adelie  Torgersen              39.1             18.7              181.0   \n",
       "1    Adelie  Torgersen              39.5             17.4              186.0   \n",
       "2    Adelie  Torgersen              40.3             18.0              195.0   \n",
       "3    Adelie  Torgersen               NaN              NaN                NaN   \n",
       "4    Adelie  Torgersen              36.7             19.3              193.0   \n",
       "..      ...        ...               ...              ...                ...   \n",
       "339  Gentoo     Biscoe               NaN              NaN                NaN   \n",
       "340  Gentoo     Biscoe              46.8             14.3              215.0   \n",
       "341  Gentoo     Biscoe              50.4             15.7              222.0   \n",
       "342  Gentoo     Biscoe              45.2             14.8              212.0   \n",
       "343  Gentoo     Biscoe              49.9             16.1              213.0   \n",
       "\n",
       "     body_mass_g     sex  \n",
       "0         3750.0    MALE  \n",
       "1         3800.0  FEMALE  \n",
       "2         3250.0  FEMALE  \n",
       "3            NaN     NaN  \n",
       "4         3450.0  FEMALE  \n",
       "..           ...     ...  \n",
       "339          NaN     NaN  \n",
       "340       4850.0  FEMALE  \n",
       "341       5750.0    MALE  \n",
       "342       5200.0  FEMALE  \n",
       "343       5400.0    MALE  \n",
       "\n",
       "[344 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_FILEPATH)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d06e0-b711-4e3d-b424-6fa611a51f94",
   "metadata": {},
   "source": [
    "## Step 3 - Preprocessing the Dataset\n",
    "\n",
    "Let's create a script to do feature engineering on the original dataset. This script should also split the data into train, validation, and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb6ba7c0-1bd6-4fe5-8b7f-f6cbdfd3846c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguins/preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile penguins/preprocessor.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "BASE_DIR = \"/opt/ml/processing\"\n",
    "DATA_FILEPATH = Path(BASE_DIR) / \"input\" / \"data.csv\"\n",
    "\n",
    "\n",
    "def save_splits(base_dir, train, validation, test):\n",
    "    \"\"\"\n",
    "    Saves the supplied datasets to disk.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_path = Path(base_dir) / \"train\" \n",
    "    validation_path = Path(base_dir) / \"validation\" \n",
    "    test_path = Path(base_dir) / \"test\"\n",
    "    \n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    validation_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(validation_path / \"validation.csv\", header=False, index=False)\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "def preprocess(base_dir, data_filepath):\n",
    "    \"\"\"\n",
    "    Preprocesses the supplied raw dataset and splits it into a train, validation,\n",
    "    and a test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(data_filepath)\n",
    "    \n",
    "    numerical_columns = [column for column in df.columns if df[column].dtype in [\"int64\", \"float64\"]]\n",
    "\n",
    "    numerical_preprocessor = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_preprocessor = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"numerical\", numerical_preprocessor, numerical_columns),\n",
    "            (\"categorical\", categorical_preprocessor, [\"island\"]),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "\n",
    "    X = df.drop([\"sex\"], axis=1)\n",
    "    columns = list(X.columns)\n",
    "    \n",
    "    X = X.to_numpy()\n",
    "    \n",
    "    np.random.shuffle(X)\n",
    "    train, validation, test = np.split(X, [int(.7 * len(X)), int(.85 * len(X))])\n",
    "    \n",
    "    X_train = pd.DataFrame(train, columns=columns)\n",
    "    X_validation = pd.DataFrame(validation, columns=columns)\n",
    "    X_test = pd.DataFrame(test, columns=columns)\n",
    "    \n",
    "    y_train = X_train.species\n",
    "    y_validation = X_validation.species\n",
    "    y_test = X_test.species\n",
    "\n",
    "    X_train.drop([\"species\"], axis=1, inplace=True)\n",
    "    X_validation.drop([\"species\"], axis=1, inplace=True)\n",
    "    X_test.drop([\"species\"], axis=1, inplace=True)\n",
    "    \n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_validation = preprocessor.transform(X_validation)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_validation = label_encoder.transform(y_validation)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "    \n",
    "    \n",
    "    train = np.concatenate((X_train, np.expand_dims(y_train, axis=1)), axis=1)\n",
    "    validation = np.concatenate((X_validation, np.expand_dims(y_validation, axis=1)), axis=1)\n",
    "    test = np.concatenate((X_test, np.expand_dims(y_test, axis=1)), axis=1)\n",
    "    \n",
    "    save_splits(base_dir, train, validation, test)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(BASE_DIR, DATA_FILEPATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214fbdcb-8ee8-4104-bcd3-d7329693299e",
   "metadata": {},
   "source": [
    "## Step 4 - Testing the Preprocessing Script\n",
    "\n",
    "We can now load the script we just created and run it locally to ensure it creates the 3 splits. \n",
    "\n",
    "Having a way to run scripts locally is crucial to shorten the development feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1f122a4-acff-4687-91b9-bfef13567d88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: ['train', 'validation', 'test']\n",
      "Train: ['train.csv']\n",
      "Validation: ['validation.csv']\n",
      "Test: ['test.csv']\n"
     ]
    }
   ],
   "source": [
    "from penguins.preprocessor import preprocess\n",
    "\n",
    "with tempfile.TemporaryDirectory() as directory:\n",
    "    preprocess(\n",
    "        base_dir=directory, \n",
    "        data_filepath=DATA_FILEPATH\n",
    "    )\n",
    "    \n",
    "    print(f\"Splits: {os.listdir(directory)}\")\n",
    "    print(f\"Train: {os.listdir(Path(directory) / 'train')}\")\n",
    "    print(f\"Validation: {os.listdir(Path(directory) / 'validation')}\")\n",
    "    print(f\"Test: {os.listdir(Path(directory) / 'test')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3938a1d-d5bd-4967-ad65-a945d5df1f61",
   "metadata": {},
   "source": [
    "## Step 5 - Pipeline Configuration\n",
    "\n",
    "When we create a SageMaker Pipeline we can specify a list of paramaters that we can use throughout the individual pipeline steps. To read more about these parameters, check [Pipeline Parameters](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-parameters.html).\n",
    "\n",
    "These are the parameters that will use in our pipeline:\n",
    "\n",
    "* `dataset_location`: This parameter represents the location of the dataset in S3. We will use this parameter during the preprocessing step to access the dataset.\n",
    "* `preprocessor_destination`: We need to define the location where the preprocessing step will be storing the dataset splits to avoid SageMaker from appending a timestamp to their auto-generated location. If we let SageMaker use a timestamp, we can't cache this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86de7edd-18b0-40d1-ac8e-0f3ef4469be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_location = ParameterString(\n",
    "    name=\"dataset_location\",\n",
    "    default_value=INPUT_DATA_URI,\n",
    ")\n",
    "\n",
    "preprocessor_destination = ParameterString(\n",
    "    name=\"preprocessor_destination\",\n",
    "    default_value=f'{S3_FILEPATH}/preprocessing',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b42440-b3c0-4ded-b578-52e21f5fbfdd",
   "metadata": {},
   "source": [
    "## Step 6 - Caching Pipeline Steps\n",
    "\n",
    "While you are building your pipeline, you don't want to rerun every step of the process unless you expect a different result. Instead, you can instruct SageMaker to reuse the result of a previous successful run of a pipeline step.\n",
    "\n",
    "You can accomplish this by caching your steps. You can find more information about this topic in [Caching Pipeline Steps](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-caching.html).\n",
    "\n",
    "Getting caching to work is tricky, and you will find SageMaker missing the cache frequently. Whenever that happens, you need to dig and figure out how to adjust the step configuration to prevent SageMaker from autogenerating data that prevents a cache hit. For example, to cache the preprocessing step we need to define the destination of the processing job to prevent SageMaker from using an autogenerated timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7e0a332-e54e-4e57-97c7-9b54f4058738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll use this cache configuration to cache individual steps for \n",
    "# a maximum of 15 days.\n",
    "cache_config = CacheConfig(\n",
    "    enable_caching=True, \n",
    "    expire_after=\"15d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9771e08-5560-492a-845e-f06988b6ae1b",
   "metadata": {},
   "source": [
    "## Step 7 - Setting up a Processing Step\n",
    "\n",
    "The first step we need in our pipeline is a [Processing Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing) to run the preprocessing script. Check the [ProcessingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep) SageMaker's SDK documentation for more information.\n",
    "\n",
    "To run our script, we need access to Scikit-Learn, so we can use the [SKLearnProcessor](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html#scikit-learn-processor) processor that comes out-of-the-box with the SageMaker's Python SDK.\n",
    "\n",
    "The input of this step will be the dataset location, and the output will be the location of the three sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b982bc3-28f0-473f-b8d2-fc35dd251e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    base_job_name=\"penguins-preprocessing\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "preprocess_step = ProcessingStep(\n",
    "    name=\"penguins-preprocessing-step\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=dataset_location, destination=\"/opt/ml/processing/input\"),  \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\", destination=preprocessor_destination),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\", destination=preprocessor_destination),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\", destination=preprocessor_destination),\n",
    "    ],\n",
    "    code=\"penguins/preprocessor.py\",\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1465690a-67b5-4d68-a026-dd874ee569e8",
   "metadata": {},
   "source": [
    "## Step 8 - Defining and Running the Pipeline\n",
    "\n",
    "We can now define and run the SageMaker Pipeline. Check [Pipeline Structure and Execution](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-pipeline.html) for more information about how to define a pipeline and [Run a Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/run-pipeline.html) for information about how to run it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "564fe6fb-00aa-480f-a58f-3144b2e33a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session1_pipeline = Pipeline(\n",
    "    name=\"session1-penguins-pipeline\",\n",
    "    parameters=[\n",
    "        dataset_location, \n",
    "        preprocessor_destination,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_step, \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d0ac8-1439-4329-9a1c-c43eafacd0da",
   "metadata": {},
   "source": [
    "Submit the pipeline definition to the SageMaker Pipelines service to create a pipeline if it doesn't exist, or update the pipeline if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4410f2f-a17b-4272-abd2-e092e134174c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session1_pipeline.upsert(role_arn=role)\n",
    "execution = session1_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032460ff-eacc-4945-ba74-8782b2d02660",
   "metadata": {},
   "source": [
    "## Step 9 - Cleaning up\n",
    "\n",
    "Before you finish, don't forget to clean up after you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83c39d12-77e1-48b5-908d-df28bf591ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:325223348818:pipeline/session1-penguins-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': 'f5e5d16b-0101-4cc7-b44c-5dfc2736a658',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'f5e5d16b-0101-4cc7-b44c-5dfc2736a658',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '94',\n",
       "   'date': 'Tue, 11 Apr 2023 20:01:15 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session1_pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f03d209-764a-4499-9467-b50e17c0976d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Assignments\n",
    "\n",
    "1. Set up an Amazon SageMaker domain using the Standard Setup. Make sure you set the network configuration to VPC Only. Create a new execution role and ensure it has access to the S3 bucket you’ll use during this class. You can also specify “Any S3 bucket” if you want this role to access every S3 bucket in your AWS account.\n",
    "\n",
    "2. Create a GitHub repository and clone it from inside SageMaker Studio. We’ll use this repository to store the code used during this program.\n",
    "\n",
    "3. Configure your SageMaker Studio session to store your name and email address and cache your credentials. You can use the following commands from a Terminal window:\n",
    "\n",
    "```bash\n",
    "$ git config --global user.name \"John Doe\"\n",
    "$ git config --global user.email johndoe@example.com\n",
    "$ git config --global credential.helper store\n",
    "```\n",
    "\n",
    "4. Prepare the MNIST dataset. Throughout the course, you will create a SageMaker pipeline to work with the MNIST dataset. MNIST is popular and relatively small, so it's easy to find pre-packaged versions of it. We aren't going to use those. Instead, we will simulate a practical scenario where the data is stored in the filesystem. To accomplish this, you will load a pre-packaged version of MNIST, save it to disk, and upload it to an S3 bucket. Complete the section \"Prepare the MNIST Dataset\" below.\n",
    "\n",
    "5. Setup a SageMaker Pipeline with a preprocessing step where you split the MNIST dataset into a train and a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b110ce9c-deb9-40d6-8072-fdfe70612a28",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare the MNIST Dataset\n",
    "\n",
    "These are the steps you need to follow to prepare the data:\n",
    "\n",
    "1. Create the S3 bucket to upload the dataset.\n",
    "2. Load the MNIST dataset from the Keras built-in collection of small datasets, convert it into images, and save them to the disk.\n",
    "3. Upload the dataset to the S3 bucket.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f85d52e-9ee1-4d10-a86c-ee3393f3dcfb",
   "metadata": {},
   "source": [
    "#### Create the dataset\n",
    "\n",
    "We want to load the Keras' built-in version of MNIST and save it to disk so we can later upload it to S3.\n",
    "\n",
    "There are 70,000 images. Running this cell will take some time, so this is the perfect moment to walk around and grab some coffee. Fortunately, we only need to do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a1e59b-152c-4d8c-9e74-a263091e0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "\n",
    "def save(dataset, split, images, labels):\n",
    "    \"\"\"\n",
    "    This function saves the handwritten digits to disk as PNG files.\n",
    "    \n",
    "    Every image will be saved inside a folder corresponding to \n",
    "    its label. For example, a digit from the train set representing \n",
    "    the number 3 will be saved inside the folder `~/train/3`.\n",
    "    \"\"\"\n",
    "    \n",
    "    for index, (image, label) in enumerate(zip(images, labels)):\n",
    "        im = Image.fromarray(image)\n",
    "\n",
    "        path = dataset / split / str(label)\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        im.save(path / f\"{index}.png\")\n",
    "        \n",
    "\n",
    "# We will save the dataset in the home directory, inside a folder\n",
    "# named `dataset`.\n",
    "dataset = Path.home() / \"dataset\" \n",
    "\n",
    "# We want to make sure we don't generate the images if the dataset\n",
    "# already exists.\n",
    "if not dataset.exists():\n",
    "    # Load the MNIST dataset using the Keras library. This returns the\n",
    "    # dataset in numpy arrays.\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Use the function we created to save the data to disk.\n",
    "    save(dataset, split=\"train\", images=X_train, labels=y_train)\n",
    "    save(dataset, split=\"test\", images=X_test, labels=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6983d2-3db2-41b9-8ea0-8d3ab336e353",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Upload the data to S3\n",
    "\n",
    "Now that we exported the MNIST dataset to the filesystem, we need to upload them to an S3 bucket. The easiest way to do this is to use the AWS CLI.\n",
    "\n",
    "This command will also take a while to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27de16c-602c-4d00-a404-3e539930f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $dataset s3://$BUCKET/mnist --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696bed04-048c-4956-9016-b8a76519bd8c",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Amazon SageMaker is free to try. Your free tier starts from the first month when you create your first SageMaker resource and lasts 2 months. Check out the [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/) for more information.\n",
    "\n",
    "2. We’ll be working extensively with [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html) and [SageMaker’s Python SDK](https://sagemaker.readthedocs.io/en/stable/). Keep their documentation handy.\n",
    "\n",
    "3. Check the [SageMaker Pipelines Overview](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) for an introduction to the fundamental components of a SageMaker Pipeline.\n",
    "\n",
    "4. Check [Pipeline Parameters](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-parameters.html) for more information on how to define and use variables in your pipeline.\n",
    "\n",
    "5. Check [Caching Pipeline Steps](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-caching.html) for information on how to cache the results of individual pipeline steps.\n",
    "\n",
    "6. Check the [ProcessingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep) SageMaker's SDK documentation. You can find an example of how to create a processing job from the pipeline in the [Processing Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing) page.\n",
    "\n",
    "7. Check [Pipeline Structure and Execution](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-pipeline.html) for more information about how to define a pipeline.\n",
    "\n",
    "8. Check [Run a Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/run-pipeline.html) for information about how to submit the pipeline definition to the SageMaker Pipelines service to create a pipeline if it doesn't exist, or update the pipeline if it does, and then run it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c940d74-d409-4425-a796-1186a34c3532",
   "metadata": {},
   "source": [
    "## Additional Notes\n",
    "\n",
    "1. This notebook uses a Scikit-Learn Pipeline to transform the dataset. You should always orchestrate your transformations using pipelines. Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) for more details.\n",
    "\n",
    "2. The preprocessing script uses `np.split()` to split the dataset into 3 different splits. It's a neat way of getting the three splits with a single instruction.\n",
    "\n",
    "3. Keras offers a [list of built-in vectorized datasets](https://www.notion.so/Bnomial-RESTful-API-4ecf85043b484ec994d7f70c56abfe27) in NumPy format. You can load any of these datasets with a single line of code, making them convenient.\n",
    "\n",
    "4. Converting a Numpy array into an image you can save and visualize is a useful trick to know. Check the `Image.fromarray()` function from the `PIL` library.\n",
    "\n",
    "5. The [command line interface](https://docs.aws.amazon.com/cli/latest/index.html) is a simple way to interact with the AWS services. You can combine Python code with bash commands in the same notebook cell, which makes notebooks a very flexible tool.\n",
    "\n",
    "6. Check Python’s `pathlib` module. Since Python 3.4, this module offers a clean way to interact with the filesystem.\n",
    "\n",
    "7. This notebook uses the `%%writefile`, `%load_ext`, and `%autoreload` magics. These magics are very useful when using notebooks. Check this list of [line and cell magics](https://ipython.readthedocs.io/en/stable/interactive/magics.html) for other examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c190c5-52b5-4ccc-8d42-847a694b8e66",
   "metadata": {},
   "source": [
    "# Session 2 - Training and Tuning\n",
    "\n",
    "This session extends the [SageMaker Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) we built in the previous session with one step that trains a model.\n",
    "\n",
    "We explore the [Training](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training) and the [Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-tuning) steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4756a3d8-d47b-4a88-a7f2-65ed7d4c1175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "from sagemaker.parameter import IntegerParameter\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8608092-7aab-4fd2-aa99-47c2db27bdb7",
   "metadata": {},
   "source": [
    "## Step 1 - Training the Model\n",
    "\n",
    "This script is responsible from training a simple neural network on the train data, validating the model, and saving it so we can later use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d92b121d-dcb9-43e8-9ee3-3ececb583e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguins/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile penguins/train.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "def train(base_directory, train_path, validation_path, epochs=50, batch_size=32):\n",
    "    X_train = pd.read_csv(Path(train_path) / \"train.csv\")\n",
    "    y_train = X_train[X_train.columns[-1]]\n",
    "    X_train.drop(X_train.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "    X_validation = pd.read_csv(Path(validation_path) / \"validation.csv\")\n",
    "    y_validation = X_validation[X_validation.columns[-1]]\n",
    "    X_validation.drop(X_validation.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(10, input_shape=(X_train.shape[1],), activation=\"relu\"),\n",
    "        Dense(8, activation=\"relu\"),\n",
    "        Dense(3, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=SGD(learning_rate=0.01),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        validation_data=(X_validation, y_validation),\n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    predictions = np.argmax(model.predict(X_validation), axis=-1)\n",
    "    print(f\"Validation accuracy: {accuracy_score(y_validation, predictions)}\")\n",
    "    \n",
    "    model_filepath = Path(base_directory) / \"model\" / \"001\"\n",
    "    model.save(model_filepath)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Any hyperparameters provided by the training job are passed to the entry point\n",
    "    # as script arguments. SageMaker will also provide a list of special parameters\n",
    "    # that you can capture here. Here is the full list: \n",
    "    # https://github.com/aws/sagemaker-training-toolkit/blob/master/src/sagemaker_training/params.py\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--base_directory\", type=str, default=\"/opt/ml/\")\n",
    "    parser.add_argument(\"--train_path\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\", None))\n",
    "    parser.add_argument(\"--validation_path\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\", None))\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    train(\n",
    "        base_directory=args.base_directory,\n",
    "        train_path=args.train_path,\n",
    "        validation_path=args.validation_path,\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f0a4fa-ce70-4882-b9f5-8253df03d890",
   "metadata": {},
   "source": [
    "## Step 2 - Testing the Training Script\n",
    "\n",
    "Let's test the script we just created by running it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14ea27ce-c453-4cb0-b309-dbecd732957e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Extension horovod.torch has not been built: /usr/local/lib/python3.8/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-38-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still avaiable.\n",
      "[2023-04-11 20:01:49.420 tensorflow-2-6-cpu-py-ml-t3-medium-9169b2e75617c45c79c40579f6a8:236 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-11 20:01:49.510 tensorflow-2-6-cpu-py-ml-t3-medium-9169b2e75617c45c79c40579f6a8:236 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "8/8 - 1s - loss: 1.0310 - accuracy: 0.5314 - val_loss: 0.9874 - val_accuracy: 0.6078\n",
      "Epoch 2/50\n",
      "8/8 - 0s - loss: 0.9778 - accuracy: 0.6067 - val_loss: 0.9337 - val_accuracy: 0.7451\n",
      "Epoch 3/50\n",
      "8/8 - 0s - loss: 0.9282 - accuracy: 0.6653 - val_loss: 0.8834 - val_accuracy: 0.7647\n",
      "Epoch 4/50\n",
      "8/8 - 0s - loss: 0.8820 - accuracy: 0.7364 - val_loss: 0.8358 - val_accuracy: 0.8627\n",
      "Epoch 5/50\n",
      "8/8 - 0s - loss: 0.8380 - accuracy: 0.7782 - val_loss: 0.7907 - val_accuracy: 0.8627\n",
      "Epoch 6/50\n",
      "8/8 - 0s - loss: 0.7965 - accuracy: 0.7908 - val_loss: 0.7475 - val_accuracy: 0.8431\n",
      "Epoch 7/50\n",
      "8/8 - 0s - loss: 0.7572 - accuracy: 0.8033 - val_loss: 0.7062 - val_accuracy: 0.8824\n",
      "Epoch 8/50\n",
      "8/8 - 0s - loss: 0.7198 - accuracy: 0.8117 - val_loss: 0.6682 - val_accuracy: 0.8824\n",
      "Epoch 9/50\n",
      "8/8 - 0s - loss: 0.6856 - accuracy: 0.8201 - val_loss: 0.6322 - val_accuracy: 0.8824\n",
      "Epoch 10/50\n",
      "8/8 - 0s - loss: 0.6527 - accuracy: 0.8243 - val_loss: 0.5989 - val_accuracy: 0.8824\n",
      "Epoch 11/50\n",
      "8/8 - 0s - loss: 0.6225 - accuracy: 0.8243 - val_loss: 0.5687 - val_accuracy: 0.8824\n",
      "Epoch 12/50\n",
      "8/8 - 0s - loss: 0.5948 - accuracy: 0.8410 - val_loss: 0.5406 - val_accuracy: 0.8824\n",
      "Epoch 13/50\n",
      "8/8 - 0s - loss: 0.5690 - accuracy: 0.8410 - val_loss: 0.5144 - val_accuracy: 0.8824\n",
      "Epoch 14/50\n",
      "8/8 - 0s - loss: 0.5450 - accuracy: 0.8452 - val_loss: 0.4905 - val_accuracy: 0.8824\n",
      "Epoch 15/50\n",
      "8/8 - 0s - loss: 0.5231 - accuracy: 0.8410 - val_loss: 0.4690 - val_accuracy: 0.8824\n",
      "Epoch 16/50\n",
      "8/8 - 0s - loss: 0.5033 - accuracy: 0.8494 - val_loss: 0.4495 - val_accuracy: 0.8824\n",
      "Epoch 17/50\n",
      "8/8 - 0s - loss: 0.4852 - accuracy: 0.8452 - val_loss: 0.4313 - val_accuracy: 0.8824\n",
      "Epoch 18/50\n",
      "8/8 - 0s - loss: 0.4687 - accuracy: 0.8410 - val_loss: 0.4152 - val_accuracy: 0.8824\n",
      "Epoch 19/50\n",
      "8/8 - 0s - loss: 0.4536 - accuracy: 0.8452 - val_loss: 0.4002 - val_accuracy: 0.8824\n",
      "Epoch 20/50\n",
      "8/8 - 0s - loss: 0.4396 - accuracy: 0.8494 - val_loss: 0.3865 - val_accuracy: 0.8824\n",
      "Epoch 21/50\n",
      "8/8 - 0s - loss: 0.4267 - accuracy: 0.8703 - val_loss: 0.3739 - val_accuracy: 0.8824\n",
      "Epoch 22/50\n",
      "8/8 - 0s - loss: 0.4148 - accuracy: 0.8745 - val_loss: 0.3623 - val_accuracy: 0.8824\n",
      "Epoch 23/50\n",
      "8/8 - 0s - loss: 0.4038 - accuracy: 0.8787 - val_loss: 0.3515 - val_accuracy: 0.8824\n",
      "Epoch 24/50\n",
      "8/8 - 0s - loss: 0.3935 - accuracy: 0.8787 - val_loss: 0.3416 - val_accuracy: 0.8824\n",
      "Epoch 25/50\n",
      "8/8 - 0s - loss: 0.3839 - accuracy: 0.8787 - val_loss: 0.3323 - val_accuracy: 0.8824\n",
      "Epoch 26/50\n",
      "8/8 - 0s - loss: 0.3749 - accuracy: 0.8828 - val_loss: 0.3237 - val_accuracy: 0.8824\n",
      "Epoch 27/50\n",
      "8/8 - 0s - loss: 0.3664 - accuracy: 0.8912 - val_loss: 0.3156 - val_accuracy: 0.8824\n",
      "Epoch 28/50\n",
      "8/8 - 0s - loss: 0.3585 - accuracy: 0.8954 - val_loss: 0.3081 - val_accuracy: 0.9020\n",
      "Epoch 29/50\n",
      "8/8 - 0s - loss: 0.3510 - accuracy: 0.8954 - val_loss: 0.3010 - val_accuracy: 0.9216\n",
      "Epoch 30/50\n",
      "8/8 - 0s - loss: 0.3439 - accuracy: 0.8996 - val_loss: 0.2944 - val_accuracy: 0.9216\n",
      "Epoch 31/50\n",
      "8/8 - 0s - loss: 0.3371 - accuracy: 0.9038 - val_loss: 0.2881 - val_accuracy: 0.9412\n",
      "Epoch 32/50\n",
      "8/8 - 0s - loss: 0.3305 - accuracy: 0.9121 - val_loss: 0.2820 - val_accuracy: 0.9608\n",
      "Epoch 33/50\n",
      "8/8 - 0s - loss: 0.3244 - accuracy: 0.9121 - val_loss: 0.2762 - val_accuracy: 0.9608\n",
      "Epoch 34/50\n",
      "8/8 - 0s - loss: 0.3183 - accuracy: 0.9163 - val_loss: 0.2706 - val_accuracy: 0.9608\n",
      "Epoch 35/50\n",
      "8/8 - 0s - loss: 0.3124 - accuracy: 0.9163 - val_loss: 0.2654 - val_accuracy: 0.9608\n",
      "Epoch 36/50\n",
      "8/8 - 0s - loss: 0.3069 - accuracy: 0.9205 - val_loss: 0.2604 - val_accuracy: 0.9608\n",
      "Epoch 37/50\n",
      "8/8 - 0s - loss: 0.3016 - accuracy: 0.9289 - val_loss: 0.2556 - val_accuracy: 0.9608\n",
      "Epoch 38/50\n",
      "8/8 - 0s - loss: 0.2967 - accuracy: 0.9331 - val_loss: 0.2511 - val_accuracy: 0.9608\n",
      "Epoch 39/50\n",
      "8/8 - 0s - loss: 0.2918 - accuracy: 0.9456 - val_loss: 0.2466 - val_accuracy: 0.9608\n",
      "Epoch 40/50\n",
      "8/8 - 0s - loss: 0.2869 - accuracy: 0.9456 - val_loss: 0.2423 - val_accuracy: 0.9608\n",
      "Epoch 41/50\n",
      "8/8 - 0s - loss: 0.2823 - accuracy: 0.9498 - val_loss: 0.2382 - val_accuracy: 0.9608\n",
      "Epoch 42/50\n",
      "8/8 - 0s - loss: 0.2777 - accuracy: 0.9498 - val_loss: 0.2341 - val_accuracy: 0.9608\n",
      "Epoch 43/50\n",
      "8/8 - 0s - loss: 0.2732 - accuracy: 0.9540 - val_loss: 0.2302 - val_accuracy: 0.9608\n",
      "Epoch 44/50\n",
      "8/8 - 0s - loss: 0.2690 - accuracy: 0.9582 - val_loss: 0.2265 - val_accuracy: 0.9608\n",
      "Epoch 45/50\n",
      "8/8 - 0s - loss: 0.2649 - accuracy: 0.9582 - val_loss: 0.2226 - val_accuracy: 0.9608\n",
      "Epoch 46/50\n",
      "8/8 - 0s - loss: 0.2606 - accuracy: 0.9623 - val_loss: 0.2188 - val_accuracy: 0.9608\n",
      "Epoch 47/50\n",
      "8/8 - 0s - loss: 0.2565 - accuracy: 0.9623 - val_loss: 0.2152 - val_accuracy: 0.9608\n",
      "Epoch 48/50\n",
      "8/8 - 0s - loss: 0.2524 - accuracy: 0.9623 - val_loss: 0.2118 - val_accuracy: 0.9608\n",
      "Epoch 49/50\n",
      "8/8 - 0s - loss: 0.2485 - accuracy: 0.9623 - val_loss: 0.2084 - val_accuracy: 0.9608\n",
      "Epoch 50/50\n",
      "8/8 - 0s - loss: 0.2448 - accuracy: 0.9623 - val_loss: 0.2049 - val_accuracy: 0.9608\n",
      "Validation accuracy: 0.9607843137254902\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp2knmct_p/model/001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp2knmct_p/model/001/assets\n"
     ]
    }
   ],
   "source": [
    "from penguins.preprocessor import preprocess\n",
    "from penguins.train import train\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as directory:\n",
    "    # First, we preprocess the data and create the \n",
    "    # dataset splits.\n",
    "    preprocess(\n",
    "        base_dir=directory, \n",
    "        data_filepath=DATA_FILEPATH\n",
    "    )\n",
    "\n",
    "    # Then, we train a model using the train and \n",
    "    # validation splits.\n",
    "    train(\n",
    "        base_directory=directory, \n",
    "        train_path=Path(directory) / \"train\", \n",
    "        validation_path=Path(directory) / \"validation\",\n",
    "        epochs=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a3dc26-db20-43d1-a352-8dc3b7eb7a0a",
   "metadata": {},
   "source": [
    "## Step 3 - Configuring an Estimator\n",
    "\n",
    "SageMaker uses the concept of an [Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) to handle end-to-end training and deployment tasks. For this example, we will use the [TensorFlow Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-estimator) to run the trainning script we wrote before.\n",
    "\n",
    "SageMaker will pass the list of hyperparameters defined below to the entry point of the training script as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90fe82ae-6a2c-4461-bc83-bb52d8871e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 32\n",
    "}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point=\"penguins/train.py\",\n",
    "    role=role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    py_version=\"py37\",\n",
    "    framework_version=\"2.4\",\n",
    "    script_mode=True,\n",
    "    disable_profiler=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cff4c1-6510-4d99-8ae1-cb14927b87c7",
   "metadata": {},
   "source": [
    "## Step 4 - Setting up a Training Step\n",
    "\n",
    "We can now create a [Training Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training) that we can add to the pipeline. Check the [TrainingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TrainingStep) SageMaker's SDK documentation for more information. \n",
    "\n",
    "This step will use the estimator we configured before and will receive the train and validation splits from the preprocessing step as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99e4850c-83d6-4f4e-a813-d5a3f4bb7486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_step = TrainingStep(\n",
    "    name=\"penguins-training-step\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=preprocess_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=preprocess_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    },\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797792d0-eda4-4f35-b26c-ef6f2309a309",
   "metadata": {},
   "source": [
    "## Step 5 - Running the Pipeline with the Training Step\n",
    "\n",
    "We can now define and run the SageMaker Pipeline, this time using the new Training Step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1126c623-f6ae-4e09-a6c9-28b86e6fc265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session2_training_pipeline = Pipeline(\n",
    "    name=\"session2-training-penguins-pipeline\",\n",
    "    parameters=[\n",
    "        dataset_location, \n",
    "        preprocessor_destination,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_step, \n",
    "        training_step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc444f7-3994-4729-a60e-bc7efd326cec",
   "metadata": {},
   "source": [
    "Submit the pipeline definition to the SageMaker Pipelines service to create a pipeline if it doesn't exist, or update the pipeline if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d7b5345-7e06-4c1a-ac54-f84dbdbe15a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "session2_training_pipeline.upsert(role_arn=role)\n",
    "execution = session2_training_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a28de-5999-4d39-b759-c0e8792ea023",
   "metadata": {},
   "source": [
    "## Step 6 - Configuring a Hyperparameter Tuner\n",
    "\n",
    "An alternative to training a model is to train many variants of the model and choose the best one. We can do this with an instance of the [HyperparameterTuner](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html) class.\n",
    "\n",
    "The tuner uses the same `Estimator` we defined to train the model, and we can specify how it should determine the best model:\n",
    "\n",
    "1. `objective_metric_name`: This is the name of the metric the tuner will use to determine the best model.\n",
    "2. `objective_type`: This is the objective of the tuner. Should it \"Minimize\" the metric or \"Maximize\" it? In this example, sice we are using the validation accuracy of the model, we want the objetive to be \"Maximize.\" If we were using the loss of the model, we would set the objetive to \"Minimize.\"\n",
    "3. `metric_definitions`: Defines how the tuner will determine the value of the metric by looking at the output logs of the training process.\n",
    "\n",
    "The tuner expects a list of the hyperparameters you want to explore. You can use subclasses of the [Parameter](https://sagemaker.readthedocs.io/en/stable/api/training/parameter.html#sagemaker.parameter.ParameterRange) class to specify different types of hyperparameters.\n",
    "\n",
    "Finally, you can control the number of jobs and how many of them will run in parallel using the following two arguments:\n",
    "* `max_jobs`: Defines the maximum total number of training jobs to start for the hyperparameter tuning job.\n",
    "* `max_parallel_jobs`: Defines the maximum number of parallel training jobs to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "038ff2e5-ed28-445b-bc03-4e996ec2286f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    \"epochs\": IntegerParameter(10, 50)\n",
    "}\n",
    "\n",
    "objective_metric_name = \"val_accuracy\"\n",
    "objective_type = \"Maximize\"\n",
    "metric_definitions = [{\"Name\": objective_metric_name, \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"}]\n",
    "    \n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    objective_type=objective_type,\n",
    "    max_jobs=3,\n",
    "    max_parallel_jobs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5814e258-c633-4e9a-85c5-6ed0f168b503",
   "metadata": {},
   "source": [
    "## Step 7 - Setting up a Tuning Step\n",
    "\n",
    "We can now create a [Tuning Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-tuning) to add it to our pipeline. Check the [TuningStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TuningStep) SageMaker's SDK documentation for more information. \n",
    "\n",
    "This step will use the tuner we configured before and will receive the train and validation splits from the preprocessing step as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf5dd9a7-8643-4fbb-8eb4-40f39011e27b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuning_step = TuningStep(\n",
    "    name = \"penguins-tuning-step\",\n",
    "    tuner=tuner,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=preprocess_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=preprocess_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    },\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4babe38c-1682-42d2-8442-101d17aa89b5",
   "metadata": {},
   "source": [
    "## Step 8 - Running the Pipeline with the Tuning Step\n",
    "\n",
    "We can now define and run the SageMaker Pipeline, this time using the new Tuning Step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9799ab39-fcae-41f4-a68b-85ab71b3ba9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session2_tuning_pipeline = Pipeline(\n",
    "    name=\"session2-tuning-penguins-pipeline\",\n",
    "    parameters=[\n",
    "        dataset_location, \n",
    "        preprocessor_destination,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_step, \n",
    "        tuning_step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e0708c-3692-4d09-a269-bbd14f7b0226",
   "metadata": {},
   "source": [
    "Submit the pipeline definition to the SageMaker Pipelines service to create a pipeline if it doesn't exist, or update the pipeline if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77c6d523-6810-4bc7-8fc4-64ae978cf245",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "session2_tuning_pipeline.upsert(role_arn=role)\n",
    "execution = session2_tuning_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17942235-e050-4157-b9dd-47de56433855",
   "metadata": {},
   "source": [
    "## Step 9 - Cleaning up\n",
    "\n",
    "Before you finish, don't forget to clean up after you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0140f856-182d-4459-9b6c-45c4a8e69de8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:325223348818:pipeline/session2-tuning-penguins-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': 'fb4b7e41-1fb4-4077-887b-15ca1bf09910',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'fb4b7e41-1fb4-4077-887b-15ca1bf09910',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '101',\n",
       "   'date': 'Tue, 11 Apr 2023 20:35:46 GMT'},\n",
       "  'RetryAttempts': 2}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session2_training_pipeline.delete()\n",
    "session2_tuning_pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca966b85-5bef-4a59-a967-27bdfff47ae1",
   "metadata": {},
   "source": [
    "## Assignments\n",
    "\n",
    "1. Modify the training script so it accepts the `learning_rate` as a new hyperparameter using the list of hyperparameters supplied to the Estimator.\n",
    "\n",
    "2. Replace the TensorFlow Estimator with a Pytorch Estimator. Check [this page](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#create-an-estimator) for an example of how to create a PyTorch Estimator. You'll need to create a new training script that builds a PyTorch model to solve the problem.\n",
    "\n",
    "3. Modify the tuner job to find the best `learning_rate` value between `0.01` and `0.03`. Check the [ContinuousParameter](https://sagemaker.readthedocs.io/en/stable/api/training/parameter.html#sagemaker.parameter.ContinuousParameter) class for more information on how to configure this parameter.\n",
    "\n",
    "4. Modify the pipeline to run the training and tuning jobs concurrently.\n",
    "\n",
    "5. Modify the SageMaker Pipeline you created for the MNIST project and add a training step. That step should only receive one channel with the train data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af93846-ca69-4ae4-99d6-529c6ebda617",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. The [Docker Registry Paths and Example Code](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html) page contains information about the available fremwork versions for each region.\n",
    "\n",
    "2. Check [SageMaker Training Toolkit](https://github.com/aws/sagemaker-training-toolkit) for more information about how to train machine learning models within a Docker container using Amazon SageMaker.\n",
    "\n",
    "3. Check the [TrainingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TrainingStep) SageMaker's SDK documentation. You can find an example of how to create a training job from the pipeline in the [Training Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training) page.\n",
    "\n",
    "4. Check the [TuningStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TuningStep) SageMaker's SDK documentation. You can find an example of how to create a hyperparameter tuning job from the pipeline in the [Tuning Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-tuning) page.\n",
    "\n",
    "5. Check the [Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) and the [TensorFlow Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-estimator) documentation for more information about how these classes work. You can also find [other supported frameworks](https://sagemaker.readthedocs.io/en/stable/frameworks/index.html) in the documentation.\n",
    "\n",
    "6. Check the [HyperparameterTuner](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html) class for more information about how to configure a hyperparameter job.\n",
    "\n",
    "7. The SageMaker SDK passes special hyperparameters to the training job that we can capture from inside the script. Here is the [complete list of available hyperparameters](https://github.com/aws/sagemaker-training-toolkit/blob/master/src/sagemaker_training/params.py). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d40fe8-ba74-4c12-9555-d8ea33d1c8b4",
   "metadata": {},
   "source": [
    "# Session 3 - Evaluating the Model\n",
    "\n",
    "This session extends the [SageMaker Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) with a step to evaluate the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34fc262f-a1cf-4f94-9c60-e7c8e83cfdfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "from sagemaker.workflow.properties import PropertyFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa9691-f49f-48af-b272-3d4d17563b01",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1 - Evaluating the Model\n",
    "\n",
    "This script is reponsible from loading the model we created and evaluating it on the test set. Before finishing, this script will create a file containing an evaluation report of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee3ab26-afa5-4ceb-9f7a-005d5fdea646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguins/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile penguins/evaluation.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "MODEL_PATH = \"/opt/ml/processing/model/\"\n",
    "TEST_PATH = \"/opt/ml/processing/test/\"\n",
    "OUTPUT_PATH = \"/opt/ml/processing/evaluation/\"\n",
    "\n",
    "\n",
    "def evaluate(model_path, test_path, output_path):\n",
    "    # The first step is to extract the model package provided\n",
    "    # by SageMaker.\n",
    "    with tarfile.open(Path(model_path) / \"model.tar.gz\") as tar:\n",
    "        tar.extractall(path=Path(model_path))\n",
    "        \n",
    "    # We can now load the model from disk.\n",
    "    model = keras.models.load_model(Path(model_path) / \"001\")\n",
    "    \n",
    "    X_test = pd.read_csv(Path(test_path) / \"test.csv\")\n",
    "    y_test = X_test[X_test.columns[-1]]\n",
    "    X_test.drop(X_test.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "    predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "    # Let's add the accuracy of the model to our evaluation report.\n",
    "    evaluation_report = {\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": {\n",
    "                \"value\": accuracy\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # We need to save the evaluation report to the output path.\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    with open(Path(output_path) / \"evaluation.json\", \"w\") as f:\n",
    "        f.write(json.dumps(evaluation_report))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate(\n",
    "        model_path=MODEL_PATH, \n",
    "        test_path=TEST_PATH,\n",
    "        output_path=OUTPUT_PATH\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcc79a0-adfd-4ce9-8580-5cd228c3c2d9",
   "metadata": {},
   "source": [
    "## Step 2 - Testing the Evaluation Script\n",
    "\n",
    "Let's test the script we just created by running it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2540d8-278a-4953-bc54-0469d154427d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from penguins.preprocessor import preprocess\n",
    "from penguins.train import train\n",
    "from penguins.evaluation import evaluate\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as directory:\n",
    "    # First, we preprocess the data and create the \n",
    "    # dataset splits.\n",
    "    preprocess(\n",
    "        base_dir=directory, \n",
    "        data_filepath=DATA_FILEPATH\n",
    "    )\n",
    "\n",
    "    # Then, we train a model using the train and \n",
    "    # validation splits.\n",
    "    train(\n",
    "        base_directory=directory, \n",
    "        train_path=Path(directory) / \"train\", \n",
    "        validation_path=Path(directory) / \"validation\",\n",
    "        epochs=50\n",
    "    )\n",
    "    \n",
    "    # After training a model, we need to prepare a package just like\n",
    "    # SageMaker would. This package is what the evaluation script is\n",
    "    # expecting as an input.\n",
    "    with tarfile.open(Path(directory) / \"model.tar.gz\", \"w:gz\") as tar:\n",
    "        tar.add(Path(directory) / \"model\" / \"001\", arcname=\"001\")\n",
    "        \n",
    "    \n",
    "    # We can now call the evaluation script.\n",
    "    evaluate(\n",
    "        model_path=directory, \n",
    "        test_path=Path(directory) / \"test\",\n",
    "        output_path=Path(directory) / \"evaluation\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d0b30-edf1-4b05-a570-e4f1d5a312e1",
   "metadata": {},
   "source": [
    "## Step 3 - Setting up a Processor\n",
    "\n",
    "To run the evaluation script we can use a [Processing Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing). Check the [ProcessingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep) SageMaker's SDK documentation for more information.\n",
    "\n",
    "This time, we will use a [ScriptProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ScriptProcessor) running a TensorFlow image. This will give us access to every library we need to execute the evaluation script.\n",
    "\n",
    "You can use the [sagemaker.image_utis.retrieve()](https://sagemaker.readthedocs.io/en/stable/api/utility/image_uris.html) function for generating the URI of pre-built docker images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f85bcbe6-41eb-4f3f-b57d-6cb3ab5e5106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's retrieve the image we want to use to run the\n",
    "# processing job.\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=\"2.4\",\n",
    "    py_version=\"py37\",\n",
    "    image_scope=\"training\",\n",
    "    instance_type=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "# We can now setup the processor using the URI of\n",
    "# the pre-built docker image.\n",
    "evaluation_script_processor = ScriptProcessor(\n",
    "    base_job_name=\"penguins-evaluation-processor\",\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188ba1cc-1bed-4b78-ba3c-842945bb26ea",
   "metadata": {},
   "source": [
    "## Step 4 - Configuring the Model Input\n",
    "\n",
    "One of the inputs to the Evaluation Step is the model we created. We explored two different ways to create the model: a Training Step and a Tuning Step.\n",
    "\n",
    "Here we can configure the input to the Evaluation Step based on whether we want to select the best model generated by the Tuning Step, or the model we trained using the Training Step.\n",
    "\n",
    "We can use the [get_top_model_s3_uri()](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TuningStep.get_top_model_s3_uri) function to get the model artifacts from the top performing training jobs of the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffcebf04-53af-45e1-8f40-cc205cce8d72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# By default, this notebook uses the best model from the Tuning Step.\n",
    "# You can set this variable to False if you want to use the result\n",
    "# of the Training Step.\n",
    "USE_TUNING_STEP = True\n",
    "\n",
    "# This is the input in case we want to use the best model generated\n",
    "# by the Tuning Step.\n",
    "tuning_model_input = ProcessingInput(\n",
    "    source=tuning_step.get_top_model_s3_uri(\n",
    "        top_k=0, \n",
    "        s3_bucket=sagemaker_session.default_bucket()\n",
    "    ),\n",
    "    destination=\"/opt/ml/processing/model\",\n",
    ")\n",
    "\n",
    "# This is the input in case we want to use the trained model\n",
    "# from the Training Step.\n",
    "training_model_input = ProcessingInput(\n",
    "    source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    destination=\"/opt/ml/processing/model\"\n",
    ")\n",
    "\n",
    "# We can now select the appropriate input depending on which step\n",
    "# we are using.\n",
    "model_input = tuning_model_input if USE_TUNING_STEP else training_model_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1109a-6c26-4464-8338-94960729d212",
   "metadata": {},
   "source": [
    "## Step 5 - Setting up a Processing Step\n",
    "\n",
    "We can now create a [ProcessingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep) to run the evaluation script. We'll use the [ScriptProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ScriptProcessor) we defined before. \n",
    "\n",
    "The inputs of this step will be the model and the test set that we generated during the preprocessing step. The output will be the evaluation report file.\n",
    "\n",
    "The [ProcessingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep) lets us specify a list of [PropertyFile](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.properties.PropertyFile) instances from the output of the job. We can use this to map the evaluation report that we generate in the evaluations script. Check [How to Build and Manage Property Files](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-propertyfile.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48139a07-5c8e-4bc6-b666-bf9531f7f520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We want to map the evaluation report that we generate inside\n",
    "# the evaluation script so we can later reference it.\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"evaluation-report\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "\n",
    "# Notice how this step uses the model generated by the tuning or training\n",
    "# step, and the test set generated by the preprocessing step.\n",
    "evaluation_step = ProcessingStep(\n",
    "    name=\"penguins-evaluation-step\",\n",
    "    processor=evaluation_script_processor,\n",
    "    inputs=[\n",
    "        model_input,\n",
    "        ProcessingInput(\n",
    "            source=preprocess_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"penguins/evaluation.py\",\n",
    "    property_files=[evaluation_report],\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f8cb8-d284-425b-8c4a-f644b4ac7ebe",
   "metadata": {},
   "source": [
    "## Step 6 - Adding Model Evaluation to the Pipeline\n",
    "\n",
    "We can now add the model evaluation step to the pipeline.\n",
    "\n",
    "We are going to configure the pipeline to run the Tuning Step or the Training Step depending on the value of the `USE_TUNING_STEP` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32638959-642d-4068-8fbc-3355c618115b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session3_pipeline = Pipeline(\n",
    "    name=\"session3-penguins-pipeline\",\n",
    "    parameters=[\n",
    "        dataset_location, \n",
    "        preprocessor_destination,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_step, \n",
    "        tuning_step if USE_TUNING_STEP else training_step,\n",
    "        evaluation_step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e98fb1-fdff-4ba4-aea3-0d9f271fb24c",
   "metadata": {},
   "source": [
    "Submit the pipeline definition to the SageMaker Pipelines service to create a pipeline if it doesn't exist, or update the pipeline if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4864ab08-7acc-4719-970e-fcce66c5fef5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "session3_pipeline.upsert(role_arn=role)\n",
    "execution = session3_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520750e9-ee5f-4cd6-beae-5442dc9a75c3",
   "metadata": {},
   "source": [
    "## Step 7 - Cleaning up\n",
    "\n",
    "Before you finish, don't forget to clean up after you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf20d8-420f-4f95-8cb0-03788f7ab556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session3_pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1a43b-a58e-4424-823e-78f3806c7187",
   "metadata": {},
   "source": [
    "## Assignments\n",
    "\n",
    "1. Extend the evaluation report by adding other metrics. For example, add the support of the test set (the number of samples.)\n",
    "\n",
    "2. One of the assignments from the previous session was to replace the TensorFlow Estimator with a Pytorch Estimator. You can now modify the evaluation step to load a script that uses Pytorch to evaluate the model.\n",
    "\n",
    "3. If you are runing the Training and Tuning Steps simultaneously, create two different Evaluation Steps to evaluate both models independently.\n",
    "\n",
    "4. Instead of runing the Training and Tuning Steps simultaneously, run the Tuning Step but create two Evaluation Steps to evaluate the two best models produced by the Tuning Step. Check the [TuningJob.get_top_model_s3_uri()](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TuningStep.get_top_model_s3_uri) function to retrieve the two best models.\n",
    "\n",
    "5. Modify the SageMaker Pipeline you created for the MNIST project and add an evaluation step. That step should use the test set you generated in the preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a39ea3-a553-4762-ae42-f9272d3c3028",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Resources\n",
    "\n",
    "1. Check the [ScriptProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ScriptProcessor) for more information about how to run a processing job using a machine learning framework.\n",
    "\n",
    "2. SageMaker offers a list of pre-built docker images. You can use the [sagemaker.image_utis.retrieve()](https://sagemaker.readthedocs.io/en/stable/api/utility/image_uris.html) function for generating the URI of these images.\n",
    "\n",
    "3. You can use the [TuningJob.get_top_model_s3_uri()](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TuningStep.get_top_model_s3_uri) function to get the model artifacts from the top performing training jobs of the hyperparameter tuning job.\n",
    "\n",
    "4. Check [How to Build and Manage Property Files](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-propertyfile.html) for more information about mapping the output of a [ProcessingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep) to a [PropertyFile](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.properties.PropertyFile)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2917ee33-f895-45e6-83d3-e34255ba550b",
   "metadata": {},
   "source": [
    "# Session 4 - Deploying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6545d1a3-c1ec-4eae-92fb-5b11d8f1dbf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import ModelPackage\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.functions import Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8696f63c-c036-46be-b352-4e23b5db0553",
   "metadata": {},
   "source": [
    "## Step 1 - Preparing Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f28a689-c5ca-4373-b615-2adba4f2664a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# This represents the approval status of a new trained model in the registry.\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"model_approval_status\", \n",
    "    default_value=\"Approved\"\n",
    ")\n",
    "\n",
    "# This property represents the minimum accuracy that the model should\n",
    "# reach in order for it to be registered.\n",
    "accuracy_threshold = ParameterFloat(\n",
    "    name=\"accuracy_threshold\", \n",
    "    default_value=0.75\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96fbc5-0647-49f1-817f-85c5477e91ab",
   "metadata": {},
   "source": [
    "## Step 2 - Configuring the Model Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d6e6241-ebf7-4a02-b9eb-f30639b70f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_model_data = training_step.properties.ModelArtifacts.S3ModelArtifacts\n",
    "\n",
    "tuning_model_data = tuning_step.get_top_model_s3_uri(\n",
    "    top_k=0, \n",
    "    s3_bucket=sagemaker_session.default_bucket()\n",
    ")\n",
    "\n",
    "model_data = tuning_model_data if USE_TUNING_STEP else training_model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e705514-5931-4f36-9633-776c2305bf9a",
   "metadata": {},
   "source": [
    "## Step 3 - Configuring the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9fd8fa74-dda9-4509-bcd2-db7a30acce6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=\"2.4\",\n",
    "    image_scope=\"inference\",\n",
    "    instance_type=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_data,\n",
    "    sagemaker_session=PipelineSession(),\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441fac3a-5ecc-441f-84c3-717c4c7ba290",
   "metadata": {},
   "source": [
    "## Step 4 - Setting up a Model Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "52a48cef-fb78-412b-a5c6-977eafe98e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package_group_name = \"penguins-model-package-group\"\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=Join(on=\"\", values=[\n",
    "            evaluation_step.arguments['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri'],\n",
    "            \"/evaluation.json\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "model_step = ModelStep(\n",
    "    name=\"penguins-model-step\",\n",
    "    step_args=model.register(\n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"text/csv\"],\n",
    "        inference_instances=[\"ml.m5.large\"],\n",
    "        model_package_group_name=model_package_group_name,\n",
    "        model_metrics=model_metrics,\n",
    "        approval_status=model_approval_status,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c110f7-fe72-4db8-9d06-cfb9a0f2bfbd",
   "metadata": {},
   "source": [
    "## Step 5 - Setting up a Conditional Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "36e2a2b1-6711-4266-95d8-d2aebd52e199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "condition_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"metrics.accuracy.value\"\n",
    "    ),\n",
    "    right=accuracy_threshold\n",
    ")\n",
    "\n",
    "register_model_step = ConditionStep(\n",
    "    name=\"penguins-register-model-step\",\n",
    "    conditions=[condition_gte],\n",
    "    if_steps=[model_step],\n",
    "    else_steps=[], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309b8fa-f03e-4959-853f-dc2416f82bdd",
   "metadata": {},
   "source": [
    "## Step 6 - Adding Model Registration to the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f70bcd33-b499-4e2b-953e-94d1ed96c10a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session4_pipeline = Pipeline(\n",
    "    name=\"session4-penguins-pipeline\",\n",
    "    parameters=[\n",
    "        dataset_location, \n",
    "        preprocessor_destination,\n",
    "        model_approval_status,\n",
    "        accuracy_threshold,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_step, \n",
    "        tuning_step if USE_TUNING_STEP else training_step, \n",
    "        evaluation_step,\n",
    "        register_model_step\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "45b01065-5ba4-4c5c-9735-859b5234614a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n"
     ]
    }
   ],
   "source": [
    "session4_pipeline.upsert(role_arn=role)\n",
    "execution = session4_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502a7e9-e6fa-4706-8294-7abd51f85d07",
   "metadata": {},
   "source": [
    "## Step 7 - Loading the Latest Approved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff299800-7509-48b4-a343-e18ac4a30a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "\n",
    "def get_latest_approved_model_package(model_package_group_name):\n",
    "    \"\"\"\n",
    "    Returns the latest approved model package registered under the \n",
    "    supplied model package group.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = sagemaker_client.list_model_packages(\n",
    "            ModelPackageGroupName=model_package_group_name,\n",
    "            ModelApprovalStatus=\"Approved\",\n",
    "            SortBy=\"CreationTime\",\n",
    "            MaxResults=100,\n",
    "        )\n",
    "        approved_packages = response[\"ModelPackageSummaryList\"]\n",
    "\n",
    "        while len(approved_packages) == 0 and \"NextToken\" in response:\n",
    "            response = sagemaker_client.list_model_packages(\n",
    "                ModelPackageGroupName=model_package_group_name,\n",
    "                ModelApprovalStatus=\"Approved\",\n",
    "                SortBy=\"CreationTime\",\n",
    "                MaxResults=100,\n",
    "                NextToken=response[\"NextToken\"],\n",
    "            )\n",
    "            approved_packages.extend(response[\"ModelPackageSummaryList\"])\n",
    "\n",
    "        if len(approved_packages) == 0:\n",
    "            error = f\"No approved model pacakages for {model_package_group_name}\"\n",
    "            print(error)\n",
    "            raise Exception(error)\n",
    "\n",
    "        # Return the pmodel package arn\n",
    "        model_package_arn = approved_packages[0][\"ModelPackageArn\"]\n",
    "        logger.info(f\"Identified the latest approved model package: {model_package_arn}\")\n",
    "        return approved_packages[0]\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "        raise Exception(e.response[\"Error\"][\"Message\"])\n",
    "        \n",
    "        \n",
    "def does_endpoint_exist(endpoint_name):\n",
    "    \"\"\"\n",
    "    Returns whether the supplied endpoint already exists.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b8c80-aba9-40ce-99c8-ea7abe5f6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "approved_model_package = get_latest_approved_model_package(model_package_group_name)\n",
    "approved_model_package_arn = approved_model_package[\"ModelPackageArn\"]\n",
    "\n",
    "model_description = sagemaker_client.describe_model_package(\n",
    "    ModelPackageName=approved_model_package_arn\n",
    ")\n",
    "\n",
    "model_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb161e-ee0a-468d-9af9-136ab91ea1db",
   "metadata": {},
   "source": [
    "## Step 8 - Deploying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f71c0-183c-4018-a563-4b4ac5dd7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package = ModelPackage(\n",
    "    role=role, \n",
    "    model_package_arn=approved_model_package_arn, \n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "endpoint_name = \"penguins-endpoint\"\n",
    "\n",
    "if not does_endpoint_exist(endpoint_name):\n",
    "    model_package.deploy(\n",
    "        initial_instance_count=1, \n",
    "        instance_type=\"ml.m5.large\", \n",
    "        endpoint_name=endpoint_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27605ac3-3127-437d-ae8c-faf7de305e2c",
   "metadata": {},
   "source": [
    "## Step 9 - Testing the Model Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a6861-86e1-4e97-8f69-db2bb6469eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(endpoint_name=endpoint_name)\n",
    "\n",
    "payload = \"0.6569590202313976, -1.0813829646495108, 1.2097102831892812, 0.9226343641317372, 1.0, 0.0, 0.0\"\n",
    "p = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\n",
    "\n",
    "predictions = json.loads(p.decode(\"utf-8\"))[\"predictions\"]\n",
    "\n",
    "print(f\"Prediction: {np.argmax(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4c712-4502-408a-a6d2-d5d9b4debedb",
   "metadata": {},
   "source": [
    "## Step 11 - Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "690a5e01-face-4e06-abfc-5573a2d2b902",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:325223348818:pipeline/session4-penguins-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': '0dccb461-d532-4e0e-a21c-957e31558a4f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '0dccb461-d532-4e0e-a21c-957e31558a4f',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '94',\n",
       "   'date': 'Tue, 11 Apr 2023 22:12:41 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model_package in sagemaker_client.list_model_packages(\n",
    "    ModelPackageGroupName=model_package_group_name)[\"ModelPackageSummaryList\"]:\n",
    "    print(f\"Deleting {model_package['ModelPackageArn']}\")\n",
    "    sagemaker_client.delete_model_package(ModelPackageName=model_package[\"ModelPackageArn\"])\n",
    "\n",
    "    \n",
    "sagemaker_client.delete_model_package_group(ModelPackageGroupName=model_package_group_name)\n",
    "predictor.delete_endpoint()\n",
    "session4_pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d944aa-35ad-4f55-9852-667e27d4878a",
   "metadata": {},
   "source": [
    "## Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6b0ee-b369-4230-9cc0-a3b6f7b5d06f",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565cf77e-7fc7-406e-a2e2-40c553f459f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Session 5 - Custom Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b006dec-3da3-4a3f-a35a-d65b86ca54e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf3e0e3c-4e85-41f7-a2ec-086d874a0b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container version: 20230412a9c6\n"
     ]
    }
   ],
   "source": [
    "VERSION = \"1\"\n",
    "\n",
    "ENDPOINTS_FOLDER = \"version\" + VERSION\n",
    "REPOSITORY_NAME = \"penguins\"\n",
    "\n",
    "CONTAINER_VERSION = f\"{datetime.now().year}{str(datetime.now().month).zfill(2)}{str(datetime.now().day).zfill(2)}{str(uuid.uuid4())[-4:]}\" \n",
    "print(f\"Container version: {CONTAINER_VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4aa6dfb-f42f-4b48-85f0-f334f3823ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'penguins' already exists in the registry with id '325223348818'\n"
     ]
    }
   ],
   "source": [
    "!aws ecr create-repository --repository-name $REPOSITORY_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e301c463-da74-4e51-8aef-09b49a4ba53f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECR Repository: 325223348818.dkr.ecr.us-east-1.amazonaws.com/penguins\n"
     ]
    }
   ],
   "source": [
    "repository = !aws ecr describe-repositories \\\n",
    "    --repository-names $REPOSITORY_NAME \\\n",
    "    --query \"repositories[0].repositoryUri\"\n",
    "\n",
    "repository_uri = repository[0][1:-1]\n",
    "repository = repository_uri[0:repository_uri.index(\"/\")]\n",
    "print(f\"ECR Repository: {repository}/{REPOSITORY_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dfd42e5-f612-466e-9799-f80d64a6f115",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd ~/ml.school/container\n",
      "aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 325223348818.dkr.ecr.us-east-1.amazonaws.com\n",
      "docker build -t 325223348818.dkr.ecr.us-east-1.amazonaws.com/penguins:20230412a9c6 .\n"
     ]
    }
   ],
   "source": [
    "AWS_ACCOUNT_ID = !aws sts get-caller-identity --query \"Account\" --output text\n",
    "AWS_ACCOUNT_ID = AWS_ACCOUNT_ID[0] \n",
    "\n",
    "\n",
    "print(f\"cd ~/ml.school/container\")\n",
    "print(f\"aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {AWS_ACCOUNT_ID}.dkr.ecr.{region}.amazonaws.com\")\n",
    "print(f\"docker build -t {repository_uri}:{CONTAINER_VERSION} .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3d43440d-bc06-498c-8dbf-349ef2271ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: docker: not found\n"
     ]
    }
   ],
   "source": [
    "ecr = f\"{AWS_ACCOUNT_ID}.dkr.ecr.{region}.amazonaws.com\"\n",
    "\n",
    "!aws ecr get-login-password --region $region | docker login --username AWS --password-stdin $ecr\n",
    "\n",
    "\n",
    "# docker build --build-arg MULTI_MODEL=false -t $repository_uri:$CONTAINER_VERSION .\n",
    "    \n",
    "# aws ecr get-login-password | docker login --username AWS --password-stdin $repository\n",
    "# docker push $repository_uri:$CONTAINER_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efcebb0e-3a82-4627-bbbf-7fc1c3118a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: sdocker: not found\n"
     ]
    }
   ],
   "source": [
    "!sdocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba05dc4d-5463-40dd-9ecb-13890e1cf5a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-east-1'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a22752c-ac52-406c-ae30-faf7fd2ebd27",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "* [SageMaker Inference Toolkit](https://github.com/aws/sagemaker-inference-toolkit)\n",
    "* [Amazon SageMaker Model Monitor](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_monitoring.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a625a-7dba-4e77-a572-7198f18ef399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.6-cpu-py38-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
