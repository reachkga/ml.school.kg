{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad79da9b-34c1-4bfe-bfd2-7c9405303df9",
   "metadata": {},
   "source": [
    "# Session 2\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e867f26-7e14-40e0-ac04-ec72d3f2b302",
   "metadata": {},
   "source": [
    "### Sagemaker Training Jobs\n",
    "\n",
    "- TensorFlow Estimators\n",
    "- The train.py script\n",
    "- Inputs\n",
    "- Hyperparameters\n",
    "- Saving the model\n",
    "- Training Logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c838be80-74d1-41c0-baa6-a9854ae22b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Conv2D, Dense, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dropout\n",
    "\n",
    "\n",
    "def train(base_directory, epochs=10, batch_size=32):\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    X_train = (X_train - 0.0) / (255.0 - 0.0)\n",
    "    X_test = (X_test - 0.0) / (255.0 - 0.0)\n",
    "\n",
    "    X_train = X_train.reshape((X_train.shape + (1,)))\n",
    "    X_test = X_test.reshape((X_test.shape + (1,)))\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(100, activation=\"relu\"),\n",
    "        Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss=\"sparse_categorical_crossentropy\", \n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "    \n",
    "    model_filepath = os.path.join(base_directory, \"model\", \"0001\")\n",
    "    model.save(model_filepath)\n",
    "    \n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--base_directory\", type=str, default=\"/opt/ml/\")\n",
    "parser.add_argument(\"--train_folder\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\", None))\n",
    "parser.add_argument(\"--validation_folder\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\", None))\n",
    "parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "# args.train_folder is where the train data is\n",
    "# args.validation_folder is where the validation data is\n",
    "\n",
    "train(\n",
    "    base_directory=args.base_directory,\n",
    "    epochs=args.epochs,\n",
    "    batch_size=args.batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67219535-80cb-446e-ae33-d9546f74de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "from utils import to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cea19a2-d975-4f14-a0be-367c3a71096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURATION_DIRECTORY = \"/tmp/training\"\n",
    "\n",
    "!mkdir -p $CONFIGURATION_DIRECTORY\n",
    "!cp train.py $CONFIGURATION_DIRECTORY\n",
    "!cp requirements.txt $CONFIGURATION_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5802b10-60a9-49bd-8109-29930dbf6c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requirements.txt  train.py\n"
     ]
    }
   ],
   "source": [
    "!ls $CONFIGURATION_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c8dd360-c6fd-47c8-80ae-b2d3c9e38d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: sample-training-2023-02-07-15-08-13-159\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 32\n",
    "}\n",
    "\n",
    "inputs = {\n",
    "    # \"train\": \"s3://MyBucketName/dataset/train\",\n",
    "    # \"validation\": \"s3://MyBucketName/dataset/validation\",\n",
    "}\n",
    "\n",
    "## Esta es la configuraciÃ³n de un Training Job.\n",
    "estimator = TensorFlow(\n",
    "    base_job_name=\"sample-training\",\n",
    "    source_dir=CONFIGURATION_DIRECTORY,\n",
    "    entry_point=\"train.py\",\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    py_version=\"py37\",\n",
    "    framework_version=\"2.4\",\n",
    "    debugger_hook_config=False,\n",
    "    script_mode=True,\n",
    "    volume_size=5,\n",
    ")\n",
    "\n",
    "estimator.fit(inputs=None, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2f135-7a56-4b9f-aaec-830749aabc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner = HyperparameterTuner(\n",
    "#     estimator,\n",
    "#     objective_metric_name,\n",
    "#     hyperparamter_range,\n",
    "#     metric_definitions,\n",
    "#     max_jobs=3,\n",
    "#     max_parallel_jobs=3,\n",
    "#     objective_type=objective_type,\n",
    "# )\n",
    "\n",
    "# tuner.fit(inputs=channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a39de7-9bf0-4581-8002-618105037e7c",
   "metadata": {},
   "source": [
    "### Deploying model using the same estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a1d9387-7fd8-4956-b7ea-958ad912cf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker.tensorflow.model:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating model with name: sample-training-2023-02-07-15-15-01-530\n",
      "INFO:sagemaker:Creating endpoint-config with name sample-training-2023-02-07-15-15-01-530\n",
      "INFO:sagemaker:Creating endpoint with name sample-training-2023-02-07-15-15-01-530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type=\"ml.c5.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6480c63d-9957-4ca8-9380-3a845a2e580b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.png ---> 7\n",
      "1.png ---> 1\n",
      "2.png ---> 2\n"
     ]
    }
   ],
   "source": [
    "for image_path in os.listdir(\"./images\"):\n",
    "    if image_path.endswith(\".png\"):\n",
    "        image = to_tensor(os.path.join(\"./images\", image_path))\n",
    "        \n",
    "        result = predictor.predict(image)\n",
    "        prediction = np.argmax(result[\"predictions\"])\n",
    "        print(f\"{image_path} ---> {prediction}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4614c778-4be0-43f5-abc8-91ab4d050f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: sample-training-2023-02-07-15-15-01-530\n",
      "INFO:sagemaker:Deleting endpoint with name: sample-training-2023-02-07-15-15-01-530\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b02ff-c9a4-48f5-ba6c-0104eda8d1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
