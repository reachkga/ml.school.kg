{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The content\n",
    "\n",
    "1. Setup dev environment\n",
    "1. Setup MLFlow\n",
    "\n",
    "1. Explain the training pipeline\n",
    "Diagram: Training Flow\n",
    "This lesson acts like an introduction to what we'll be building.\n",
    "This pipeline covers up to model registration.\n",
    "\n",
    "\n",
    "1. Introduction to metaflow. \n",
    "Create a sample metaflow flow. \n",
    "Explain how to run it from the command line and from a notebook.\n",
    "This session is necessary so we can start running every step as part of a flow.\n",
    "\n",
    "\n",
    "1. Load the data.\n",
    "We create a function to do this. \n",
    "Unit test this function.\n",
    "Create a flow that does it.\n",
    "\n",
    "\n",
    "1. Cross-validation.\n",
    "Diagram: cross-validation process.\n",
    "Explain why we'll be using cross-validation.\n",
    "Explain general structure of a cross-validation process: split, transform, train, evaluate, and then final evaluation.\n",
    "Explain metaflow's foreach.\n",
    "Set up flow: cross-validation(split) -> transform_fold -> train_fold -> evaluate_fold -> evaluation\n",
    "Implement the split part.\n",
    "\n",
    "\n",
    "1. Transform the data. \n",
    "Explain steps to transform the data.\n",
    "Integrate transforming the data into the flow.\n",
    "\n",
    "\n",
    "1. Training.\n",
    "Explain how to train the model.\n",
    "Integrate in the flow.\n",
    "Keras with JAX backend.\n",
    "Explain metaflow env variables\n",
    "\n",
    "\n",
    "1. Introduction to Experiment tracking\n",
    "Update training step to track metrics in mlflow\n",
    "\n",
    "\n",
    "1. Evaluating fold\n",
    "Explain how to evaluate the model.\n",
    "Integrate this in the flow.\n",
    "log metrics in mlflow\n",
    "\n",
    "\n",
    "1. Evaluate cross-validated model\n",
    "Explain how to do this\n",
    "Integrate this in the flow.\n",
    "log metrics in mlflow\n",
    "\n",
    "\n",
    "1. Train final model using all of the data\n",
    "Add transform entire dataset to flow\n",
    "Add train model to flow\n",
    "log metrics in mlflow\n",
    "\n",
    "\n",
    "1. Introduction to model versioning\n",
    "Implement Registration in the flow\n",
    "\n",
    "\n",
    "1. The need for a Custom model\n",
    "Implement the custom model class\n",
    "Update registration to use this class\n",
    "\n",
    "\n",
    "## Considerations:\n",
    "\n",
    "1. Have an EDA flow\n",
    "1. Create a new flow for Hyperparameter tuning. The result of this flow will be the best set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_FOLDER = Path(\"code\")\n",
    "CODE_FOLDER.mkdir(exist_ok=True)\n",
    "sys.path.extend([f\"./{CODE_FOLDER}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/load.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/load.py\n",
    "\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from metaflow import S3\n",
    "\n",
    "\n",
    "def load_data_from_s3(location: str):\n",
    "    \"\"\"Load the dataset from an S3 location.\n",
    "\n",
    "    This function will concatenate every CSV file in the given location\n",
    "    and return a single DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset from location {location}\")\n",
    "\n",
    "    with S3(s3root=location) as s3:\n",
    "        files = s3.get_all()\n",
    "\n",
    "        print(f\"Found {len(files)} file(s) in remote location\")\n",
    "\n",
    "        raw_data = [pd.read_csv(StringIO(file.text)) for file in files]\n",
    "        return pd.concat(raw_data)\n",
    "\n",
    "\n",
    "def load_data_from_file():\n",
    "    \"\"\"Load the dataset from a local file.\n",
    "\n",
    "    This function is useful to test the pipeline locally\n",
    "    without having to access the data remotely.\n",
    "    \"\"\"\n",
    "    location = Path(\"../../penguins.csv\")\n",
    "    print(f\"Loading dataset from location {location.as_posix()}\")\n",
    "    return pd.read_csv(location)\n",
    "\n",
    "\n",
    "def load_data(bucket, dataset_location, debug=False):\n",
    "    if debug:\n",
    "        df = load_data_from_file()\n",
    "    else:\n",
    "        location = f\"s3://{bucket}/{dataset_location}\"\n",
    "        df = load_data_from_s3(location)\n",
    "\n",
    "    # Shuffle the data\n",
    "    data = df.sample(frac=1, random_state=42)\n",
    "\n",
    "    print(f\"Loaded dataset with {len(data)} samples\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load import load_data\n",
    "\n",
    "data = load_data(\"mlschool\", \"penguins.csv\", debug=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metaflow 2.12.3 executing MyFlow2 for user:svpino\n",
      "Validating your flow...\n",
      "    The graph looks good!\n",
      "Running pylint...\n",
      "    Pylint not found, so extra checks are disabled.\n",
      "Bootstrapping virtual environment(s) ...\n",
      "Virtual environment(s) bootstrapped!\n",
      "2024-07-18 13:01:58.127 Workflow starting (run-id 1721300518120064):\n",
      "2024-07-18 13:01:58.162 [1721300518120064/start/1 (pid 87062)] Task is starting.\n",
      "2024-07-18 13:01:58.345 [1721300518120064/start/1 (pid 87062)] Task finished successfully.\n",
      "2024-07-18 13:01:58.375 [1721300518120064/load_data/2 (pid 87073)] Task is starting.\n",
      "2024-07-18 13:01:58.673 [1721300518120064/load_data/2 (pid 87073)] Loading dataset from location ../../penguins.csv\n",
      "2024-07-18 13:01:58.675 [1721300518120064/load_data/2 (pid 87073)] Loaded dataset with 344 samples\n",
      "2024-07-18 13:01:58.678 [1721300518120064/load_data/2 (pid 87073)] species  island  ...  body_mass_g     sex\n",
      "2024-07-18 13:01:58.717 [1721300518120064/load_data/2 (pid 87073)] 194  Chinstrap   Dream  ...       3550.0    MALE\n",
      "2024-07-18 13:01:58.717 [1721300518120064/load_data/2 (pid 87073)] 157  Chinstrap   Dream  ...       3950.0  FEMALE\n",
      "2024-07-18 13:01:58.717 [1721300518120064/load_data/2 (pid 87073)] 225     Gentoo  Biscoe  ...       4550.0  FEMALE\n",
      "2024-07-18 13:01:58.717 [1721300518120064/load_data/2 (pid 87073)] 208  Chinstrap   Dream  ...       3250.0  FEMALE\n",
      "2024-07-18 13:01:58.718 [1721300518120064/load_data/2 (pid 87073)] 318     Gentoo  Biscoe  ...       4625.0  FEMALE\n",
      "2024-07-18 13:01:58.718 [1721300518120064/load_data/2 (pid 87073)] \n",
      "2024-07-18 13:01:58.718 [1721300518120064/load_data/2 (pid 87073)] [5 rows x 7 columns]\n",
      "2024-07-18 13:01:58.718 [1721300518120064/load_data/2 (pid 87073)] Task finished successfully.\n",
      "2024-07-18 13:01:58.752 [1721300518120064/end/3 (pid 87118)] Task is starting.\n",
      "2024-07-18 13:01:58.896 [1721300518120064/end/3 (pid 87118)] the end\n",
      "2024-07-18 13:01:58.921 [1721300518120064/end/3 (pid 87118)] Task finished successfully.\n",
      "2024-07-18 13:01:58.921 Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from metaflow import FlowSpec, NBRunner, Parameter, pypi_base, step, pypi\n",
    "\n",
    "\n",
    "PACKAGES = {\n",
    "    \"python\": \"3.10.14\",\n",
    "    \"packages\": {\n",
    "        \"python-dotenv\": \"1.0.1\",\n",
    "        \"pandas\": \"2.2.2\",\n",
    "        \"numpy\": \"1.26.4\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# @pypi_base(**PACKAGES)\n",
    "class MyFlow2(FlowSpec):\n",
    "    @step\n",
    "    def start(self):\n",
    "        self.next(self.load_data)\n",
    "\n",
    "    @pypi(packages={\"pandas\": \"2.2.2\"})\n",
    "    @step\n",
    "    def load_data(self):\n",
    "        from load import load_data\n",
    "\n",
    "        data = load_data(\"mlschool\", \"penguins.csv\", debug=True)\n",
    "        print(data.head())\n",
    "\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        print(\"the end\")\n",
    "\n",
    "\n",
    "run = NBRunner(MyFlow2, base_dir=\"code\", environment=\"pypi\").nbrun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metaflow 2.12.3 executing MyFlow2 for user:svpino\n",
      "Validating your flow...\n",
      "    The graph looks good!\n",
      "Running pylint...\n",
      "    Pylint not found, so extra checks are disabled.\n",
      "Bootstrapping virtual environment(s) ...\n",
      "Virtual environment(s) bootstrapped!\n",
      "2024-07-18 13:15:29.103 Workflow starting (run-id 1721301329101247):\n",
      "2024-07-18 13:15:29.136 [1721301329101247/start/1 (pid 5624)] Task is starting.\n",
      "2024-07-18 13:15:29.292 [1721301329101247/start/1 (pid 5624)] Task finished successfully.\n",
      "2024-07-18 13:15:29.321 [1721301329101247/load_data/2 (pid 5629)] Task is starting.\n",
      "2024-07-18 13:15:29.643 [1721301329101247/load_data/2 (pid 5629)] Loading dataset from location ../../penguins.csv\n",
      "2024-07-18 13:15:29.644 [1721301329101247/load_data/2 (pid 5629)] Loaded dataset with 344 samples\n",
      "2024-07-18 13:15:29.648 [1721301329101247/load_data/2 (pid 5629)] species  island  ...  body_mass_g     sex\n",
      "2024-07-18 13:15:29.694 [1721301329101247/load_data/2 (pid 5629)] 194  Chinstrap   Dream  ...       3550.0    MALE\n",
      "2024-07-18 13:15:29.694 [1721301329101247/load_data/2 (pid 5629)] 157  Chinstrap   Dream  ...       3950.0  FEMALE\n",
      "2024-07-18 13:15:29.694 [1721301329101247/load_data/2 (pid 5629)] 225     Gentoo  Biscoe  ...       4550.0  FEMALE\n",
      "2024-07-18 13:15:29.694 [1721301329101247/load_data/2 (pid 5629)] 208  Chinstrap   Dream  ...       3250.0  FEMALE\n",
      "2024-07-18 13:15:29.694 [1721301329101247/load_data/2 (pid 5629)] 318     Gentoo  Biscoe  ...       4625.0  FEMALE\n",
      "2024-07-18 13:15:29.695 [1721301329101247/load_data/2 (pid 5629)] \n",
      "2024-07-18 13:15:29.695 [1721301329101247/load_data/2 (pid 5629)] [5 rows x 7 columns]\n",
      "2024-07-18 13:15:29.695 [1721301329101247/load_data/2 (pid 5629)] Task finished successfully.\n",
      "2024-07-18 13:15:29.727 [1721301329101247/step2/3 (pid 5634)] Task is starting.\n",
      "2024-07-18 13:15:29.877 [1721301329101247/step2/3 (pid 5634)] Second step here!\n",
      "2024-07-18 13:15:29.902 [1721301329101247/step2/3 (pid 5634)] Task finished successfully.\n",
      "2024-07-18 13:15:29.936 [1721301329101247/end/4 (pid 5639)] Task is starting.\n",
      "2024-07-18 13:15:30.086 [1721301329101247/end/4 (pid 5639)] the end\n",
      "2024-07-18 13:15:30.107 [1721301329101247/end/4 (pid 5639)] Task finished successfully.\n",
      "2024-07-18 13:15:30.109 Done!\n"
     ]
    }
   ],
   "source": [
    "from metaflow import FlowSpec, NBRunner, Parameter, step, pypi\n",
    "\n",
    "\n",
    "class MyFlow2(FlowSpec):\n",
    "    @step\n",
    "    def start(self):\n",
    "        self.next(self.load_data)\n",
    "\n",
    "    @pypi(packages={\"pandas\": \"2.2.2\"})\n",
    "    @step\n",
    "    def load_data(self):\n",
    "        from load import load_data\n",
    "\n",
    "        data = load_data(\"mlschool\", \"penguins.csv\", debug=True)\n",
    "        print(data.head())\n",
    "\n",
    "        self.next(self.step2)\n",
    "\n",
    "    @step\n",
    "    def step2(self):\n",
    "        print(\"Second step here!\")\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        print(\"the end\")\n",
    "\n",
    "\n",
    "run = NBRunner(MyFlow2, base_dir=\"code\", environment=\"pypi\").nbrun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metaflow 2.12.3 executing TrainingFlow for user:svpino\n",
      "Validating your flow...\n",
      "    The graph looks good!\n",
      "Running pylint...\n",
      "    Pylint not found, so extra checks are disabled.\n",
      "Bootstrapping virtual environment(s) ...\n",
      "Virtual environment(s) bootstrapped!\n",
      "2024-07-18 20:17:15.307 Workflow starting (run-id 1721326635302940):\n",
      "2024-07-18 20:17:15.344 [1721326635302940/start/1 (pid 10409)] Task is starting.\n",
      "2024-07-18 20:17:16.375 [1721326635302940/start/1 (pid 10409)] Task finished successfully.\n",
      "2024-07-18 20:17:16.407 [1721326635302940/load_data/2 (pid 10459)] Task is starting.\n",
      "2024-07-18 20:17:17.102 [1721326635302940/load_data/2 (pid 10459)] Loading dataset from location s3://mlschool/metaflow/data/\n",
      "2024-07-18 20:17:18.646 [1721326635302940/load_data/2 (pid 10459)] Found 1 file(s) in remote location\n",
      "2024-07-18 20:17:18.649 [1721326635302940/load_data/2 (pid 10459)] Loaded dataset with 344 samples\n",
      "2024-07-18 20:17:18.736 [1721326635302940/load_data/2 (pid 10459)] Task finished successfully.\n",
      "2024-07-18 20:17:18.769 [1721326635302940/prepare_dataset/3 (pid 10513)] Task is starting.\n",
      "2024-07-18 20:17:19.417 [1721326635302940/prepare_dataset/3 (pid 10513)] Task finished successfully.\n",
      "2024-07-18 20:17:19.449 [1721326635302940/transform/4 (pid 10518)] Task is starting.\n",
      "2024-07-18 20:17:20.401 [1721326635302940/transform/4 (pid 10518)] Task finished successfully.\n",
      "2024-07-18 20:17:20.434 [1721326635302940/cross_validation/5 (pid 10563)] Task is starting.\n",
      "2024-07-18 20:17:21.338 [1721326635302940/cross_validation/5 (pid 10563)] Foreach yields 5 child steps.\n",
      "2024-07-18 20:17:21.338 [1721326635302940/cross_validation/5 (pid 10563)] Task finished successfully.\n",
      "2024-07-18 20:17:21.371 [1721326635302940/transform_fold/6 (pid 10568)] Task is starting.\n",
      "2024-07-18 20:17:22.322 [1721326635302940/transform_fold/6 (pid 10568)] Task finished successfully.\n",
      "2024-07-18 20:17:22.356 [1721326635302940/transform_fold/7 (pid 10613)] Task is starting.\n",
      "2024-07-18 20:17:23.272 [1721326635302940/transform_fold/7 (pid 10613)] Task finished successfully.\n",
      "2024-07-18 20:17:23.306 [1721326635302940/transform_fold/8 (pid 10618)] Task is starting.\n",
      "2024-07-18 20:17:24.281 [1721326635302940/transform_fold/8 (pid 10618)] Task finished successfully.\n",
      "2024-07-18 20:17:24.314 [1721326635302940/transform_fold/9 (pid 10663)] Task is starting.\n",
      "2024-07-18 20:17:25.260 [1721326635302940/transform_fold/9 (pid 10663)] Task finished successfully.\n",
      "2024-07-18 20:17:25.291 [1721326635302940/transform_fold/10 (pid 10669)] Task is starting.\n",
      "2024-07-18 20:17:26.221 [1721326635302940/transform_fold/10 (pid 10669)] Task finished successfully.\n",
      "2024-07-18 20:17:26.253 [1721326635302940/train_model_fold/11 (pid 10714)] Task is starting.\n",
      "2024-07-18 20:17:26.795 [1721326635302940/train_model_fold/11 (pid 10714)] Training fold 0...\n",
      "2024-07-18 20:17:27.184 [1721326635302940/train_model_fold/11 (pid 10714)] 2024/07/18 20:17:27 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2024-07-18 20:17:30.116 [1721326635302940/train_model_fold/11 (pid 10714)] Task finished successfully.\n",
      "2024-07-18 20:17:30.151 [1721326635302940/train_model_fold/12 (pid 10813)] Task is starting.\n",
      "2024-07-18 20:17:30.701 [1721326635302940/train_model_fold/12 (pid 10813)] Training fold 1...\n",
      "2024-07-18 20:17:31.039 [1721326635302940/train_model_fold/12 (pid 10813)] 2024/07/18 20:17:31 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2024-07-18 20:17:33.960 [1721326635302940/train_model_fold/12 (pid 10813)] Task finished successfully.\n",
      "2024-07-18 20:17:33.994 [1721326635302940/train_model_fold/13 (pid 10872)] Task is starting.\n",
      "2024-07-18 20:17:34.642 [1721326635302940/train_model_fold/13 (pid 10872)] Training fold 2...\n",
      "2024-07-18 20:17:34.999 [1721326635302940/train_model_fold/13 (pid 10872)] 2024/07/18 20:17:34 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2024-07-18 20:17:37.981 [1721326635302940/train_model_fold/13 (pid 10872)] Task finished successfully.\n",
      "2024-07-18 20:17:38.012 [1721326635302940/train_model_fold/14 (pid 10967)] Task is starting.\n",
      "2024-07-18 20:17:38.625 [1721326635302940/train_model_fold/14 (pid 10967)] Training fold 3...\n",
      "2024-07-18 20:17:38.975 [1721326635302940/train_model_fold/14 (pid 10967)] 2024/07/18 20:17:38 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2024-07-18 20:17:41.951 [1721326635302940/train_model_fold/14 (pid 10967)] Task finished successfully.\n",
      "2024-07-18 20:17:41.984 [1721326635302940/train_model_fold/15 (pid 11061)] Task is starting.\n",
      "2024-07-18 20:17:42.535 [1721326635302940/train_model_fold/15 (pid 11061)] Training fold 4...\n",
      "2024-07-18 20:17:42.913 [1721326635302940/train_model_fold/15 (pid 11061)] 2024/07/18 20:17:42 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2024-07-18 20:17:45.907 [1721326635302940/train_model_fold/15 (pid 11061)] Task finished successfully.\n",
      "2024-07-18 20:17:45.946 [1721326635302940/evaluate_model_fold/16 (pid 11155)] Task is starting.\n",
      "2024-07-18 20:17:46.563 [1721326635302940/evaluate_model_fold/16 (pid 11155)] Evaluating fold 0...\n",
      "2024-07-18 20:17:47.311 [1721326635302940/evaluate_model_fold/16 (pid 11155)] 3/3 - 0s - 52ms/step - accuracy: 0.8841 - loss: 0.3267\n",
      "2024-07-18 20:17:47.320 [1721326635302940/evaluate_model_fold/16 (pid 11155)] Fold 0 - loss: 0.32671263813972473 - accuracy: 0.8840579986572266\n",
      "2024-07-18 20:17:47.465 [1721326635302940/evaluate_model_fold/16 (pid 11155)] Task finished successfully.\n",
      "2024-07-18 20:17:47.501 [1721326635302940/evaluate_model_fold/17 (pid 11201)] Task is starting.\n",
      "2024-07-18 20:17:48.167 [1721326635302940/evaluate_model_fold/17 (pid 11201)] Evaluating fold 1...\n",
      "2024-07-18 20:17:48.889 [1721326635302940/evaluate_model_fold/17 (pid 11201)] 3/3 - 0s - 48ms/step - accuracy: 0.9275 - loss: 0.2886\n",
      "2024-07-18 20:17:48.898 [1721326635302940/evaluate_model_fold/17 (pid 11201)] Fold 1 - loss: 0.2885616421699524 - accuracy: 0.9275362491607666\n",
      "2024-07-18 20:17:49.040 [1721326635302940/evaluate_model_fold/17 (pid 11201)] Task finished successfully.\n",
      "2024-07-18 20:17:49.074 [1721326635302940/evaluate_model_fold/18 (pid 11247)] Task is starting.\n",
      "2024-07-18 20:17:49.676 [1721326635302940/evaluate_model_fold/18 (pid 11247)] Evaluating fold 2...\n",
      "2024-07-18 20:17:50.387 [1721326635302940/evaluate_model_fold/18 (pid 11247)] 3/3 - 0s - 49ms/step - accuracy: 1.0000 - loss: 0.3370\n",
      "2024-07-18 20:17:50.395 [1721326635302940/evaluate_model_fold/18 (pid 11247)] Fold 2 - loss: 0.3370083272457123 - accuracy: 1.0\n",
      "2024-07-18 20:17:50.528 [1721326635302940/evaluate_model_fold/18 (pid 11247)] Task finished successfully.\n",
      "2024-07-18 20:17:50.560 [1721326635302940/evaluate_model_fold/19 (pid 11253)] Task is starting.\n",
      "2024-07-18 20:17:51.143 [1721326635302940/evaluate_model_fold/19 (pid 11253)] Evaluating fold 3...\n",
      "2024-07-18 20:17:51.856 [1721326635302940/evaluate_model_fold/19 (pid 11253)] 3/3 - 0s - 48ms/step - accuracy: 0.9855 - loss: 0.1775\n",
      "2024-07-18 20:17:51.863 [1721326635302940/evaluate_model_fold/19 (pid 11253)] Fold 3 - loss: 0.17754344642162323 - accuracy: 0.9855072498321533\n",
      "2024-07-18 20:17:52.003 [1721326635302940/evaluate_model_fold/19 (pid 11253)] Task finished successfully.\n",
      "2024-07-18 20:17:52.036 [1721326635302940/evaluate_model_fold/20 (pid 11299)] Task is starting.\n",
      "2024-07-18 20:17:52.634 [1721326635302940/evaluate_model_fold/20 (pid 11299)] Evaluating fold 4...\n",
      "2024-07-18 20:17:53.342 [1721326635302940/evaluate_model_fold/20 (pid 11299)] 3/3 - 0s - 47ms/step - accuracy: 1.0000 - loss: 0.0793\n",
      "2024-07-18 20:17:53.350 [1721326635302940/evaluate_model_fold/20 (pid 11299)] Fold 4 - loss: 0.07928244024515152 - accuracy: 1.0\n",
      "2024-07-18 20:17:53.479 [1721326635302940/evaluate_model_fold/20 (pid 11299)] Task finished successfully.\n",
      "2024-07-18 20:17:53.529 [1721326635302940/evaluate_model/21 (pid 11345)] Task is starting.\n",
      "2024-07-18 20:17:54.146 [1721326635302940/evaluate_model/21 (pid 11345)] Accuracy: 0.9594202995300293 ±0.0461952859441807\n",
      "2024-07-18 20:17:54.227 [1721326635302940/evaluate_model/21 (pid 11345)] Loss: 0.24182169884443283 ±0.09899726503641143\n",
      "2024-07-18 20:17:54.228 [1721326635302940/evaluate_model/21 (pid 11345)] Task finished successfully.\n",
      "2024-07-18 20:17:54.260 [1721326635302940/train_model/22 (pid 11350)] Task is starting.\n",
      "2024-07-18 20:17:55.182 [1721326635302940/train_model/22 (pid 11350)] 2024/07/18 20:17:55 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2024-07-18 20:17:55.478 [1721326635302940/train_model/22 (pid 11350)] Epoch 1/50\n",
      "2024-07-18 20:17:55.807 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 29ms/step - accuracy: 0.3372 - loss: 1.2053\n",
      "2024-07-18 20:17:55.807 [1721326635302940/train_model/22 (pid 11350)] Epoch 2/50\n",
      "2024-07-18 20:17:55.887 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 7ms/step - accuracy: 0.3808 - loss: 1.1512\n",
      "2024-07-18 20:17:55.887 [1721326635302940/train_model/22 (pid 11350)] Epoch 3/50\n",
      "2024-07-18 20:17:55.891 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 358us/step - accuracy: 0.4331 - loss: 1.1065\n",
      "2024-07-18 20:17:55.891 [1721326635302940/train_model/22 (pid 11350)] Epoch 4/50\n",
      "2024-07-18 20:17:55.894 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 246us/step - accuracy: 0.4622 - loss: 1.0669\n",
      "2024-07-18 20:17:55.894 [1721326635302940/train_model/22 (pid 11350)] Epoch 5/50\n",
      "2024-07-18 20:17:55.898 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 255us/step - accuracy: 0.4913 - loss: 1.0315\n",
      "2024-07-18 20:17:55.898 [1721326635302940/train_model/22 (pid 11350)] Epoch 6/50\n",
      "2024-07-18 20:17:55.900 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 227us/step - accuracy: 0.5203 - loss: 0.9990\n",
      "2024-07-18 20:17:55.901 [1721326635302940/train_model/22 (pid 11350)] Epoch 7/50\n",
      "2024-07-18 20:17:55.904 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 253us/step - accuracy: 0.5494 - loss: 0.9692\n",
      "2024-07-18 20:17:55.904 [1721326635302940/train_model/22 (pid 11350)] Epoch 8/50\n",
      "2024-07-18 20:17:55.907 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 249us/step - accuracy: 0.6424 - loss: 0.9392\n",
      "2024-07-18 20:17:55.907 [1721326635302940/train_model/22 (pid 11350)] Epoch 9/50\n",
      "2024-07-18 20:17:55.910 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 244us/step - accuracy: 0.7035 - loss: 0.9095\n",
      "2024-07-18 20:17:55.910 [1721326635302940/train_model/22 (pid 11350)] Epoch 10/50\n",
      "2024-07-18 20:17:55.913 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 220us/step - accuracy: 0.7762 - loss: 0.8806\n",
      "2024-07-18 20:17:55.913 [1721326635302940/train_model/22 (pid 11350)] Epoch 11/50\n",
      "2024-07-18 20:17:55.916 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 245us/step - accuracy: 0.8140 - loss: 0.8519\n",
      "2024-07-18 20:17:55.916 [1721326635302940/train_model/22 (pid 11350)] Epoch 12/50\n",
      "2024-07-18 20:17:55.919 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 238us/step - accuracy: 0.8488 - loss: 0.8236\n",
      "2024-07-18 20:17:55.919 [1721326635302940/train_model/22 (pid 11350)] Epoch 13/50\n",
      "2024-07-18 20:17:55.922 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 232us/step - accuracy: 0.8779 - loss: 0.7961\n",
      "2024-07-18 20:17:55.922 [1721326635302940/train_model/22 (pid 11350)] Epoch 14/50\n",
      "2024-07-18 20:17:55.925 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 221us/step - accuracy: 0.8953 - loss: 0.7699\n",
      "2024-07-18 20:17:55.925 [1721326635302940/train_model/22 (pid 11350)] Epoch 15/50\n",
      "2024-07-18 20:17:55.928 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 239us/step - accuracy: 0.9128 - loss: 0.7440\n",
      "2024-07-18 20:17:55.928 [1721326635302940/train_model/22 (pid 11350)] Epoch 16/50\n",
      "2024-07-18 20:17:55.931 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 237us/step - accuracy: 0.9157 - loss: 0.7194\n",
      "2024-07-18 20:17:55.931 [1721326635302940/train_model/22 (pid 11350)] Epoch 17/50\n",
      "2024-07-18 20:17:55.934 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 251us/step - accuracy: 0.9157 - loss: 0.6954\n",
      "2024-07-18 20:17:55.934 [1721326635302940/train_model/22 (pid 11350)] Epoch 18/50\n",
      "2024-07-18 20:17:55.937 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 248us/step - accuracy: 0.9157 - loss: 0.6721\n",
      "2024-07-18 20:17:55.937 [1721326635302940/train_model/22 (pid 11350)] Epoch 19/50\n",
      "2024-07-18 20:17:55.940 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 242us/step - accuracy: 0.9186 - loss: 0.6494\n",
      "2024-07-18 20:17:55.940 [1721326635302940/train_model/22 (pid 11350)] Epoch 20/50\n",
      "2024-07-18 20:17:55.943 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 228us/step - accuracy: 0.9186 - loss: 0.6270\n",
      "2024-07-18 20:17:55.943 [1721326635302940/train_model/22 (pid 11350)] Epoch 21/50\n",
      "2024-07-18 20:17:55.946 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 225us/step - accuracy: 0.9186 - loss: 0.6046\n",
      "2024-07-18 20:17:55.946 [1721326635302940/train_model/22 (pid 11350)] Epoch 22/50\n",
      "2024-07-18 20:17:55.949 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 271us/step - accuracy: 0.9215 - loss: 0.5821\n",
      "2024-07-18 20:17:55.949 [1721326635302940/train_model/22 (pid 11350)] Epoch 23/50\n",
      "2024-07-18 20:17:55.952 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 238us/step - accuracy: 0.9215 - loss: 0.5593\n",
      "2024-07-18 20:17:55.952 [1721326635302940/train_model/22 (pid 11350)] Epoch 24/50\n",
      "2024-07-18 20:17:55.955 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 240us/step - accuracy: 0.9244 - loss: 0.5361\n",
      "2024-07-18 20:17:55.955 [1721326635302940/train_model/22 (pid 11350)] Epoch 25/50\n",
      "2024-07-18 20:17:55.958 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 257us/step - accuracy: 0.9273 - loss: 0.5122\n",
      "2024-07-18 20:17:55.959 [1721326635302940/train_model/22 (pid 11350)] Epoch 26/50\n",
      "2024-07-18 20:17:55.962 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 238us/step - accuracy: 0.9273 - loss: 0.4883\n",
      "2024-07-18 20:17:55.962 [1721326635302940/train_model/22 (pid 11350)] Epoch 27/50\n",
      "2024-07-18 20:17:55.964 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 224us/step - accuracy: 0.9302 - loss: 0.4639\n",
      "2024-07-18 20:17:55.965 [1721326635302940/train_model/22 (pid 11350)] Epoch 28/50\n",
      "2024-07-18 20:17:55.967 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 228us/step - accuracy: 0.9390 - loss: 0.4393\n",
      "2024-07-18 20:17:55.968 [1721326635302940/train_model/22 (pid 11350)] Epoch 29/50\n",
      "2024-07-18 20:17:55.970 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 245us/step - accuracy: 0.9506 - loss: 0.4148\n",
      "2024-07-18 20:17:55.971 [1721326635302940/train_model/22 (pid 11350)] Epoch 30/50\n",
      "2024-07-18 20:17:55.973 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 237us/step - accuracy: 0.9564 - loss: 0.3905\n",
      "2024-07-18 20:17:55.974 [1721326635302940/train_model/22 (pid 11350)] Epoch 31/50\n",
      "2024-07-18 20:17:55.976 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 231us/step - accuracy: 0.9564 - loss: 0.3669\n",
      "2024-07-18 20:17:55.977 [1721326635302940/train_model/22 (pid 11350)] Epoch 32/50\n",
      "2024-07-18 20:17:55.979 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 230us/step - accuracy: 0.9622 - loss: 0.3442\n",
      "2024-07-18 20:17:55.979 [1721326635302940/train_model/22 (pid 11350)] Epoch 33/50\n",
      "2024-07-18 20:17:55.982 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 249us/step - accuracy: 0.9738 - loss: 0.3223\n",
      "2024-07-18 20:17:55.983 [1721326635302940/train_model/22 (pid 11350)] Epoch 34/50\n",
      "2024-07-18 20:17:55.986 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 242us/step - accuracy: 0.9738 - loss: 0.3014\n",
      "2024-07-18 20:17:55.986 [1721326635302940/train_model/22 (pid 11350)] Epoch 35/50\n",
      "2024-07-18 20:17:55.988 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 226us/step - accuracy: 0.9738 - loss: 0.2819\n",
      "2024-07-18 20:17:55.989 [1721326635302940/train_model/22 (pid 11350)] Epoch 36/50\n",
      "2024-07-18 20:17:55.991 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 232us/step - accuracy: 0.9738 - loss: 0.2636\n",
      "2024-07-18 20:17:55.992 [1721326635302940/train_model/22 (pid 11350)] Epoch 37/50\n",
      "2024-07-18 20:17:55.994 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 232us/step - accuracy: 0.9767 - loss: 0.2470\n",
      "2024-07-18 20:17:55.995 [1721326635302940/train_model/22 (pid 11350)] Epoch 38/50\n",
      "2024-07-18 20:17:55.997 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 230us/step - accuracy: 0.9826 - loss: 0.2317\n",
      "2024-07-18 20:17:55.997 [1721326635302940/train_model/22 (pid 11350)] Epoch 39/50\n",
      "2024-07-18 20:17:56.000 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 227us/step - accuracy: 0.9826 - loss: 0.2177\n",
      "2024-07-18 20:17:56.000 [1721326635302940/train_model/22 (pid 11350)] Epoch 40/50\n",
      "2024-07-18 20:17:56.003 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 227us/step - accuracy: 0.9826 - loss: 0.2046\n",
      "2024-07-18 20:17:56.003 [1721326635302940/train_model/22 (pid 11350)] Epoch 41/50\n",
      "2024-07-18 20:17:56.006 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 224us/step - accuracy: 0.9855 - loss: 0.1928\n",
      "2024-07-18 20:17:56.006 [1721326635302940/train_model/22 (pid 11350)] Epoch 42/50\n",
      "2024-07-18 20:17:56.009 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 235us/step - accuracy: 0.9855 - loss: 0.1820\n",
      "2024-07-18 20:17:56.009 [1721326635302940/train_model/22 (pid 11350)] Epoch 43/50\n",
      "2024-07-18 20:17:56.012 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 247us/step - accuracy: 0.9855 - loss: 0.1721\n",
      "2024-07-18 20:17:56.012 [1721326635302940/train_model/22 (pid 11350)] Epoch 44/50\n",
      "2024-07-18 20:17:56.016 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 296us/step - accuracy: 0.9855 - loss: 0.1630\n",
      "2024-07-18 20:17:56.016 [1721326635302940/train_model/22 (pid 11350)] Epoch 45/50\n",
      "2024-07-18 20:17:56.019 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 251us/step - accuracy: 0.9855 - loss: 0.1544\n",
      "2024-07-18 20:17:56.019 [1721326635302940/train_model/22 (pid 11350)] Epoch 46/50\n",
      "2024-07-18 20:17:56.022 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 247us/step - accuracy: 0.9855 - loss: 0.1467\n",
      "2024-07-18 20:17:56.022 [1721326635302940/train_model/22 (pid 11350)] Epoch 47/50\n",
      "2024-07-18 20:17:56.026 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 281us/step - accuracy: 0.9855 - loss: 0.1397\n",
      "2024-07-18 20:17:56.026 [1721326635302940/train_model/22 (pid 11350)] Epoch 48/50\n",
      "2024-07-18 20:17:56.029 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 242us/step - accuracy: 0.9855 - loss: 0.1330\n",
      "2024-07-18 20:17:56.029 [1721326635302940/train_model/22 (pid 11350)] Epoch 49/50\n",
      "2024-07-18 20:17:56.032 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 259us/step - accuracy: 0.9855 - loss: 0.1270\n",
      "2024-07-18 20:17:56.032 [1721326635302940/train_model/22 (pid 11350)] Epoch 50/50\n",
      "2024-07-18 20:17:56.035 [1721326635302940/train_model/22 (pid 11350)] 11/11 - 0s - 243us/step - accuracy: 0.9855 - loss: 0.1213\n",
      "2024-07-18 20:17:58.277 [1721326635302940/train_model/22 (pid 11350)] Task finished successfully.\n",
      "2024-07-18 20:17:58.309 [1721326635302940/register_model/23 (pid 11451)] Task is starting.\n",
      "2024-07-18 20:17:58.932 [1721326635302940/register_model/23 (pid 11451)] Registering model...\n",
      "2024-07-18 20:17:59.912 [1721326635302940/register_model/23 (pid 11451)] Registered model 'penguins' already exists. Creating a new version of this model...\n",
      "2024-07-18 20:17:59.922 [1721326635302940/register_model/23 (pid 11451)] Created version '16' of model 'penguins'.\n",
      "2024-07-18 20:18:00.118 [1721326635302940/register_model/23 (pid 11451)] Task finished successfully.\n",
      "2024-07-18 20:18:00.156 [1721326635302940/end/24 (pid 11500)] Task is starting.\n",
      "2024-07-18 20:18:00.799 [1721326635302940/end/24 (pid 11500)] the end\n",
      "2024-07-18 20:18:00.886 [1721326635302940/end/24 (pid 11500)] Task finished successfully.\n",
      "2024-07-18 20:18:00.888 Done!\n",
      "Run('TrainingFlow/1721326635302940')\n"
     ]
    }
   ],
   "source": [
    "from metaflow import Runner\n",
    "\n",
    "with Runner(\"training.py\", environment=\"pypi\", env={\"KERAS_BACKEND\": \"jax\"}).run(\n",
    "    max_workers=1\n",
    ") as running:\n",
    "    print(f\"{running.run}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  culmen_length_mm  culmen_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen              39.1             18.7              181.0   \n",
       "1  Adelie  Torgersen              39.5             17.4              186.0   \n",
       "2  Adelie  Torgersen              40.3             18.0              195.0   \n",
       "3  Adelie  Torgersen               NaN              NaN                NaN   \n",
       "4  Adelie  Torgersen              36.7             19.3              193.0   \n",
       "\n",
       "   body_mass_g     sex  \n",
       "0       3750.0    MALE  \n",
       "1       3800.0  FEMALE  \n",
       "2       3250.0  FEMALE  \n",
       "3          NaN     NaN  \n",
       "4       3450.0  FEMALE  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "location = Path(\"../penguins.csv\")\n",
    "df = pd.read_csv(location)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels = df.pop(\"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(344, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = labels.to_numpy().reshape(-1, 1)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "label_transformer = ColumnTransformer(\n",
    "    transformers=[(\"species\", OrdinalEncoder(), [0])],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = label_transformer.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "numeric_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"mean\"),\n",
    "    StandardScaler(),\n",
    ")\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"),\n",
    ")\n",
    "\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"numeric\",\n",
    "            numeric_transformer,\n",
    "            make_column_selector(dtype_exclude=\"object\"),\n",
    "        ),\n",
    "        (\n",
    "            \"categorical\",\n",
    "            categorical_transformer,\n",
    "            [\"island\"],\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.88708123,  0.78774251, -1.42248782, -0.56578921,  0.        ,\n",
       "        0.        ,  1.        ])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = transformer.fit_transform(df)\n",
    "td[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(\n",
    "    [[\"Torgersen2\", 39.1, 18.7, 181.0, 3750.0, \"MALE\"]],\n",
    "    columns=list(df.columns),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.88708123,  0.78774251, -1.42248782, -0.56578921,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sex\"] = df[\"sex\"].replace(\".\", np.nan)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.sample(frac=0.2, random_state=42)\n",
    "train_df = df.drop(test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>38.9</td>\n",
       "      <td>17.8</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3625.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  culmen_length_mm  culmen_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen              39.1             18.7              181.0   \n",
       "1  Adelie  Torgersen              39.5             17.4              186.0   \n",
       "2  Adelie  Torgersen              40.3             18.0              195.0   \n",
       "4  Adelie  Torgersen              36.7             19.3              193.0   \n",
       "6  Adelie  Torgersen              38.9             17.8              181.0   \n",
       "\n",
       "   body_mass_g     sex  \n",
       "0       3750.0    MALE  \n",
       "1       3800.0  FEMALE  \n",
       "2       3250.0  FEMALE  \n",
       "4       3450.0  FEMALE  \n",
       "6       3625.0  FEMALE  "
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=2, shuffle=True)\n",
    "kfold_indices = list(enumerate(kfold.split(train_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " (array([  0,   1,   3,   4,   5,   7,  10,  14,  16,  17,  18,  23,  24,\n",
       "          25,  26,  28,  29,  30,  31,  32,  35,  36,  37,  38,  40,  41,\n",
       "          43,  45,  46,  47,  50,  52,  53,  59,  60,  61,  65,  66,  67,\n",
       "          68,  69,  71,  72,  78,  79,  80,  81,  82,  84,  95,  98,  99,\n",
       "         101, 102, 103, 104, 108, 109, 111, 115, 116, 117, 119, 122, 130,\n",
       "         132, 133, 136, 137, 139, 140, 141, 143, 144, 145, 147, 149, 150,\n",
       "         152, 153, 154, 155, 160, 162, 166, 172, 173, 174, 175, 177, 178,\n",
       "         179, 187, 188, 194, 195, 196, 197, 200, 201, 202, 203, 205, 215,\n",
       "         216, 217, 219, 220, 221, 222, 224, 225, 227, 228, 230, 234, 235,\n",
       "         238, 239, 242, 244, 245, 246, 247, 248, 249, 251, 254, 255, 256,\n",
       "         261, 263, 265, 267, 268, 271, 274]),\n",
       "  array([  2,   6,   8,   9,  11,  12,  13,  15,  19,  20,  21,  22,  27,\n",
       "          33,  34,  39,  42,  44,  48,  49,  51,  54,  55,  56,  57,  58,\n",
       "          62,  63,  64,  70,  73,  74,  75,  76,  77,  83,  85,  86,  87,\n",
       "          88,  89,  90,  91,  92,  93,  94,  96,  97, 100, 105, 106, 107,\n",
       "         110, 112, 113, 114, 118, 120, 121, 123, 124, 125, 126, 127, 128,\n",
       "         129, 131, 134, 135, 138, 142, 146, 148, 151, 156, 157, 158, 159,\n",
       "         161, 163, 164, 165, 167, 168, 169, 170, 171, 176, 180, 181, 182,\n",
       "         183, 184, 185, 186, 189, 190, 191, 192, 193, 198, 199, 204, 206,\n",
       "         207, 208, 209, 210, 211, 212, 213, 214, 218, 223, 226, 229, 231,\n",
       "         232, 233, 236, 237, 240, 241, 243, 250, 252, 253, 257, 258, 259,\n",
       "         260, 262, 264, 266, 269, 270, 272, 273])))"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[kfold_indices[0][1][0]]\n",
    "test_df = df.iloc[kfold_indices[0][1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import StringLookup\n",
    "\n",
    "label_lookup = StringLookup(\n",
    "    # the order here is important since the first index will be encoded as 0\n",
    "    vocabulary=[\"Adelie\", \"Chinstrap\", \"Gentoo\"],\n",
    "    num_oov_indices=0,\n",
    ")\n",
    "\n",
    "\n",
    "def encode_label(x, y):\n",
    "    encoded_y = label_lookup(y)\n",
    "    return x, encoded_y\n",
    "\n",
    "\n",
    "def dataframe_to_dataset(df):\n",
    "    df = df.copy()\n",
    "    labels = df.pop(\"species\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    ds = ds.map(encode_label, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.shuffle(buffer_size=len(df))\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_dataset = dataframe_to_dataset(train_df)\n",
    "test_dataset = dataframe_to_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'island': <tf.Tensor: shape=(), dtype=string, numpy=b'Biscoe'>, 'culmen_length_mm': <tf.Tensor: shape=(), dtype=float64, numpy=42.8>, 'culmen_depth_mm': <tf.Tensor: shape=(), dtype=float64, numpy=14.2>, 'flipper_length_mm': <tf.Tensor: shape=(), dtype=float64, numpy=209.0>, 'body_mass_g': <tf.Tensor: shape=(), dtype=float64, numpy=4700.0>, 'sex': <tf.Tensor: shape=(), dtype=string, numpy=b'FEMALE'>}\n",
      "Target: tf.Tensor(2, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 17:21:07.476200: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    print(\"Input:\", x)\n",
    "    print(\"Target:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.batch(32)\n",
    "test_dataset = test_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'island': <tf.Tensor: shape=(32,), dtype=string, numpy=\n",
      "array([b'Biscoe', b'Torgersen', b'Biscoe', b'Dream', b'Biscoe', b'Biscoe',\n",
      "       b'Torgersen', b'Torgersen', b'Dream', b'Biscoe', b'Dream',\n",
      "       b'Biscoe', b'Dream', b'Biscoe', b'Biscoe', b'Biscoe', b'Dream',\n",
      "       b'Biscoe', b'Biscoe', b'Biscoe', b'Biscoe', b'Biscoe', b'Biscoe',\n",
      "       b'Biscoe', b'Biscoe', b'Biscoe', b'Dream', b'Dream', b'Dream',\n",
      "       b'Torgersen', b'Dream', b'Biscoe'], dtype=object)>, 'culmen_length_mm': <tf.Tensor: shape=(32,), dtype=float64, numpy=\n",
      "array([44. , 41.5, 49.6, 49.8, 35.5, 43.3, 36.2, 36.2, 43.5, 39.7, 47.6,\n",
      "       45.6, 51.3, 49.9, 49.5, 46.8, 45.6, 45.2, 37.7, 47.5, 40.6, 50.8,\n",
      "       49.4, 50. , 36.5, 49.3, 50.8, 39.7, 49. , 39.7, 35.6, 42. ])>, 'culmen_depth_mm': <tf.Tensor: shape=(32,), dtype=float64, numpy=\n",
      "array([13.6, 18.3, 15. , 17.3, 16.2, 14. , 17.2, 16.1, 18.1, 17.7, 18.3,\n",
      "       20.3, 19.9, 16.1, 16.1, 14.3, 19.4, 13.8, 16. , 14.2, 18.6, 15.7,\n",
      "       15.8, 15.9, 16.6, 15.7, 19. , 17.9, 19.6, 18.4, 17.5, 19.5])>, 'flipper_length_mm': <tf.Tensor: shape=(32,), dtype=float64, numpy=\n",
      "array([208., 195., 216., 198., 195., 208., 187., 187., 202., 193., 195.,\n",
      "       191., 198., 213., 224., 215., 194., 215., 183., 209., 183., 226.,\n",
      "       216., 224., 181., 217., 210., 193., 212., 190., 191., 200.])>, 'body_mass_g': <tf.Tensor: shape=(32,), dtype=float64, numpy=\n",
      "array([4350., 4300., 4750., 3675., 3350., 4575., 3150., 3550., 3400.,\n",
      "       3200., 3850., 4600., 3700., 5400., 5650., 4850., 3525., 4750.,\n",
      "       3075., 4600., 3550., 5200., 4925., 5350., 2850., 5850., 4100.,\n",
      "       4250., 4300., 3900., 3175., 4050.])>, 'sex': <tf.Tensor: shape=(32,), dtype=string, numpy=\n",
      "array([b'FEMALE', b'MALE', b'MALE', b'FEMALE', b'FEMALE', b'FEMALE',\n",
      "       b'FEMALE', b'FEMALE', b'FEMALE', b'FEMALE', b'FEMALE', b'MALE',\n",
      "       b'MALE', b'MALE', b'MALE', b'FEMALE', b'FEMALE', b'FEMALE',\n",
      "       b'FEMALE', b'FEMALE', b'MALE', b'MALE', b'MALE', b'MALE',\n",
      "       b'FEMALE', b'MALE', b'MALE', b'MALE', b'MALE', b'MALE', b'FEMALE',\n",
      "       b'MALE'], dtype=object)>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 16:08:53.143833: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    print(\"Input:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import FeatureSpace\n",
    "\n",
    "feature_space = FeatureSpace(\n",
    "    features={\n",
    "        \"sex\": FeatureSpace.string_categorical(num_oov_indices=0),\n",
    "        \"island\": \"string_categorical\",\n",
    "        \"culmen_length_mm\": \"float_normalized\",\n",
    "        \"culmen_depth_mm\": \"float_normalized\",\n",
    "        \"flipper_length_mm\": \"float_normalized\",\n",
    "        \"body_mass_g\": \"float_normalized\",\n",
    "    },\n",
    "    output_mode=\"concat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 14:52:46.673085: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-08 14:52:46.701861: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-08 14:52:46.726271: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-08 14:52:46.750699: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-08 14:52:46.774244: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-08 14:52:46.801095: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-08 14:52:46.823662: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-08 14:52:46.850782: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-08 14:52:46.873484: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-08 14:52:46.903032: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-08 14:52:46.926288: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-08 14:52:46.953709: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "train_ds_with_no_labels = train_dataset.map(lambda x, _: x)\n",
    "feature_space.adapt(train_ds_with_no_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_x.shape: (32, 10)\n",
      "preprocessed_x.dtype: <dtype: 'float32'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 14:52:47.613255: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for x, _ in train_dataset.take(1):\n",
    "    preprocessed_x = feature_space(x)\n",
    "    print(\"preprocessed_x.shape:\", preprocessed_x.shape)\n",
    "    print(\"preprocessed_x.dtype:\", preprocessed_x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_ds = train_dataset.map(\n",
    "    lambda x, y: (feature_space(x), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "preprocessed_train_ds = preprocessed_train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "preprocessed_test_ds = test_dataset.map(\n",
    "    lambda x, y: (feature_space(x), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "preprocessed_test_ds = preprocessed_test_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_inputs = feature_space.get_inputs()\n",
    "encoded_features = feature_space.get_encoded_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'island': <KerasTensor shape=(None, 1), dtype=string, sparse=None, name=island>,\n",
       " 'sex': <KerasTensor shape=(None, 1), dtype=string, sparse=None, name=sex>,\n",
       " 'culmen_length_mm': <KerasTensor shape=(None, 1), dtype=float32, sparse=None, name=culmen_length_mm>,\n",
       " 'culmen_depth_mm': <KerasTensor shape=(None, 1), dtype=float32, sparse=None, name=culmen_depth_mm>,\n",
       " 'flipper_length_mm': <KerasTensor shape=(None, 1), dtype=float32, sparse=None, name=flipper_length_mm>,\n",
       " 'body_mass_g': <KerasTensor shape=(None, 1), dtype=float32, sparse=None, name=body_mass_g>}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor shape=(None, 10), dtype=float32, sparse=False, name=keras_tensor_110>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "x = Dense(10, activation=\"relu\")(encoded_features)\n",
    "x = Dense(8, activation=\"relu\")(x)\n",
    "outputs = Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=encoded_features, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=0.01),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = Model(inputs=dict_inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 - 0s - 23ms/step - accuracy: 0.1873 - loss: 1.2161 - val_accuracy: 0.2879 - val_loss: 1.1632\n",
      "Epoch 2/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.2322 - loss: 1.1728 - val_accuracy: 0.3636 - val_loss: 1.1302\n",
      "Epoch 3/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.2996 - loss: 1.1336 - val_accuracy: 0.3636 - val_loss: 1.1041\n",
      "Epoch 4/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.3408 - loss: 1.1017 - val_accuracy: 0.4242 - val_loss: 1.0803\n",
      "Epoch 5/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.3858 - loss: 1.0732 - val_accuracy: 0.4545 - val_loss: 1.0581\n",
      "Epoch 6/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.4120 - loss: 1.0464 - val_accuracy: 0.5152 - val_loss: 1.0365\n",
      "Epoch 7/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.4494 - loss: 1.0204 - val_accuracy: 0.5152 - val_loss: 1.0140\n",
      "Epoch 8/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.4906 - loss: 0.9946 - val_accuracy: 0.5606 - val_loss: 0.9917\n",
      "Epoch 9/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.5243 - loss: 0.9701 - val_accuracy: 0.6364 - val_loss: 0.9700\n",
      "Epoch 10/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.5581 - loss: 0.9461 - val_accuracy: 0.6667 - val_loss: 0.9471\n",
      "Epoch 11/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.5993 - loss: 0.9213 - val_accuracy: 0.6818 - val_loss: 0.9248\n",
      "Epoch 12/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.6404 - loss: 0.8972 - val_accuracy: 0.7273 - val_loss: 0.9017\n",
      "Epoch 13/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.6592 - loss: 0.8727 - val_accuracy: 0.7424 - val_loss: 0.8783\n",
      "Epoch 14/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.6891 - loss: 0.8486 - val_accuracy: 0.7424 - val_loss: 0.8538\n",
      "Epoch 15/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.7079 - loss: 0.8248 - val_accuracy: 0.7727 - val_loss: 0.8297\n",
      "Epoch 16/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.7228 - loss: 0.8006 - val_accuracy: 0.7727 - val_loss: 0.8058\n",
      "Epoch 17/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.7453 - loss: 0.7767 - val_accuracy: 0.7879 - val_loss: 0.7810\n",
      "Epoch 18/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.7790 - loss: 0.7528 - val_accuracy: 0.8182 - val_loss: 0.7570\n",
      "Epoch 19/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.8090 - loss: 0.7296 - val_accuracy: 0.8182 - val_loss: 0.7322\n",
      "Epoch 20/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.8427 - loss: 0.7062 - val_accuracy: 0.8485 - val_loss: 0.7074\n",
      "Epoch 21/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.8577 - loss: 0.6831 - val_accuracy: 0.8636 - val_loss: 0.6833\n",
      "Epoch 22/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.8689 - loss: 0.6605 - val_accuracy: 0.8788 - val_loss: 0.6594\n",
      "Epoch 23/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.8764 - loss: 0.6388 - val_accuracy: 0.9091 - val_loss: 0.6362\n",
      "Epoch 24/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.8914 - loss: 0.6173 - val_accuracy: 0.9242 - val_loss: 0.6133\n",
      "Epoch 25/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9176 - loss: 0.5963 - val_accuracy: 0.9545 - val_loss: 0.5896\n",
      "Epoch 26/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9213 - loss: 0.5756 - val_accuracy: 0.9545 - val_loss: 0.5666\n",
      "Epoch 27/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9363 - loss: 0.5553 - val_accuracy: 0.9545 - val_loss: 0.5450\n",
      "Epoch 28/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9401 - loss: 0.5351 - val_accuracy: 0.9545 - val_loss: 0.5243\n",
      "Epoch 29/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9401 - loss: 0.5163 - val_accuracy: 0.9545 - val_loss: 0.5042\n",
      "Epoch 30/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9438 - loss: 0.4974 - val_accuracy: 0.9697 - val_loss: 0.4839\n",
      "Epoch 31/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9438 - loss: 0.4794 - val_accuracy: 0.9697 - val_loss: 0.4656\n",
      "Epoch 32/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9513 - loss: 0.4620 - val_accuracy: 0.9697 - val_loss: 0.4475\n",
      "Epoch 33/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9588 - loss: 0.4452 - val_accuracy: 0.9697 - val_loss: 0.4301\n",
      "Epoch 34/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9588 - loss: 0.4287 - val_accuracy: 0.9697 - val_loss: 0.4137\n",
      "Epoch 35/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9625 - loss: 0.4129 - val_accuracy: 0.9697 - val_loss: 0.3977\n",
      "Epoch 36/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9663 - loss: 0.3974 - val_accuracy: 0.9545 - val_loss: 0.3809\n",
      "Epoch 37/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9700 - loss: 0.3818 - val_accuracy: 0.9545 - val_loss: 0.3652\n",
      "Epoch 38/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9700 - loss: 0.3671 - val_accuracy: 0.9545 - val_loss: 0.3502\n",
      "Epoch 39/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9738 - loss: 0.3530 - val_accuracy: 0.9545 - val_loss: 0.3362\n",
      "Epoch 40/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9738 - loss: 0.3394 - val_accuracy: 0.9545 - val_loss: 0.3226\n",
      "Epoch 41/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9738 - loss: 0.3262 - val_accuracy: 0.9545 - val_loss: 0.3097\n",
      "Epoch 42/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9738 - loss: 0.3137 - val_accuracy: 0.9545 - val_loss: 0.2972\n",
      "Epoch 43/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9738 - loss: 0.3011 - val_accuracy: 0.9545 - val_loss: 0.2854\n",
      "Epoch 44/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9738 - loss: 0.2890 - val_accuracy: 0.9545 - val_loss: 0.2736\n",
      "Epoch 45/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9775 - loss: 0.2774 - val_accuracy: 0.9545 - val_loss: 0.2625\n",
      "Epoch 46/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9775 - loss: 0.2665 - val_accuracy: 0.9545 - val_loss: 0.2517\n",
      "Epoch 47/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9775 - loss: 0.2553 - val_accuracy: 0.9545 - val_loss: 0.2416\n",
      "Epoch 48/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9813 - loss: 0.2448 - val_accuracy: 0.9545 - val_loss: 0.2315\n",
      "Epoch 49/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9813 - loss: 0.2347 - val_accuracy: 0.9697 - val_loss: 0.2221\n",
      "Epoch 50/50\n",
      "9/9 - 0s - 5ms/step - accuracy: 0.9813 - loss: 0.2247 - val_accuracy: 0.9697 - val_loss: 0.2129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3a63ae4a0>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    preprocessed_train_ds,\n",
    "    epochs=50,\n",
    "    validation_data=preprocessed_test_ds,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'island': <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'Biscoe', b'Torgersen', b'Torgersen'], dtype=object)>,\n",
       " 'culmen_length_mm': <tf.Tensor: shape=(3,), dtype=float32, numpy=array([48.6, 44.1, 39.1], dtype=float32)>,\n",
       " 'culmen_depth_mm': <tf.Tensor: shape=(3,), dtype=float32, numpy=array([16. , 18. , 18.7], dtype=float32)>,\n",
       " 'flipper_length_mm': <tf.Tensor: shape=(3,), dtype=float32, numpy=array([230., 210., 181.], dtype=float32)>,\n",
       " 'body_mass_g': <tf.Tensor: shape=(3,), dtype=float32, numpy=array([5800., 4000., 3750.], dtype=float32)>,\n",
       " 'sex': <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'MALE', b'FEMALE', b'MALE'], dtype=object)>}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = {\n",
    "    \"island\": [\"Biscoe\", \"Torgersen\", \"Torgersen\"],\n",
    "    \"culmen_length_mm\": [48.6, 44.1, 39.1],\n",
    "    \"culmen_depth_mm\": [16.0, 18.0, 18.7],\n",
    "    \"flipper_length_mm\": [230.0, 210.0, 181.0],\n",
    "    \"body_mass_g\": [5800.0, 4000.0, 3750.0],\n",
    "    \"sex\": [\"MALE\", \"FEMALE\", \"MALE\"],\n",
    "}\n",
    "\n",
    "input_dict = {name: tf.convert_to_tensor(value) for name, value in sample.items()}\n",
    "input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.03257361, 0.14812116, 0.81930524],\n",
       "       [0.3940005 , 0.28193894, 0.32406056],\n",
       "       [0.94649315, 0.02197206, 0.0315347 ]], dtype=float32)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = inference_model.predict(input_dict)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "\n",
    "\n",
    "def pred(p):\n",
    "    return tf.stack(\n",
    "        [\n",
    "            tf.cast(tf.math.argmax(p, axis=1), dtype=tf.float32),\n",
    "            tf.math.reduce_max(p, axis=1),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "prediction = Lambda(pred)(outputs)\n",
    "\n",
    "inference_model2 = Model(inputs=dict_inputs, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.        , 0.        , 0.        ],\n",
       "       [0.81930524, 0.3940005 , 0.94649315]], dtype=float32)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = inference_model2.predict(input_dict)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adelie', 'Chinstrap', 'Gentoo']"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_lookup.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = StringLookup(\n",
    "    vocabulary=label_lookup.get_vocabulary(),\n",
    "    invert=True,\n",
    "    num_oov_indices=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'Gentoo', b'Adelie', b'Adelie'], dtype=object)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(np.argmax(result, axis=1)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 0, 0]), array([0.81930524, 0.3940005 , 0.94649315], dtype=float32))"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(result, axis=1), np.max(result, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prediction': 'Adelie', 'confidence': 0.6},\n",
       " {'prediction': 'Gentoo', 'confidence': 0.9},\n",
       " {'prediction': 'Chinstrap', 'confidence': 0.8},\n",
       " {'prediction': 'Chinstrap', 'confidence': 0.7}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "classes = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\n",
    "\n",
    "prediction = np.array([0, 2, 1, 1])\n",
    "condifence = np.array([0.6, 0.9, 0.8, 0.7])\n",
    "\n",
    "prediction = np.vectorize(lambda x: classes[x])(prediction)\n",
    "\n",
    "[\n",
    "    {\"prediction\": p, \"confidence\": c}\n",
    "    for p, c in zip(prediction, condifence, strict=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adelie', 'Gentoo', 'Chinstrap', 'Chinstrap']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
