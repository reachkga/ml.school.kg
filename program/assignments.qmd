---
title: "Assignments"
---

TBD

### Chapter 1 - Introduction and Initial Setup

1. Run the pipeline on your environment using Local Mode (`LOCAL_MODE = True`) and then switch it to run it in SageMaker (`LOCAL_MODE = False`). After completing this assignment, your environment should be fully configured, and your pipeline should run without issues.


### Chapter 2 - Exploratory Data Analysis

1. Write code to identify any potential outliers in the dataset. An outlier is a data point that differs significantly from other observations.

1. Write code to determine whether the `sex` column has any predictive power for the `species` column.

1. Write code to visualize or quantity the predictive strength of every feature in the dataset.

1. Use the [Cleanlab](https://github.com/cleanlab/cleanlab) library to find any label errors in the dataset. [Cleanlab](https://github.com/cleanlab/cleanlab) will automatically detect any issues in the dataset.

1. Use [Amazon SageMaker Data Wrangler](https://aws.amazon.com/sagemaker/data-wrangler/) to split and transform the penguin's dataset. The goal of this assignment is for you to learn how to use a no-code tool to build the preprocessing workflow.


### Chapter 3 - Splitting and Transforming the Data

1. Modify the preprocessing script to split the dataset using stratified sampling instead of random sampling.

1. The Scikit-Learn transformation pipeline automatically excludes the `sex` column from the dataset. Modify the preprocessing script so the `sex` column remains in the dataset and it's used to train the model.

1. Use ChatGPT to generate a dataset with 500 random penguins and store the file in S3. Run the pipeline pointing the `dataset_location` parameter to the new dataset. By [overriding default parameters during a pipeline execution](https://docs.aws.amazon.com/sagemaker/latest/dg/run-pipeline.html#run-pipeline-parametrized), you can process different datasets without having to modify your code.

1. We want to run a distributed Processing Job across multiple instances. This is helpful when we want to process large amounts of data in parallel. Set up a Processing Step using two instances. When specifying the input to the Processing Step, you must set the `ProcessingInput.s3_data_distribution_type` attribute to `ShardedByS3Key`. By doing this, SageMaker will run a cluster with several instances running simultaneously and distribute the input files accordingly. For this setup to work, you must have more than one input file stored in S3. Check the [`S3DataDistributionType`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_S3DataSource.html) documentation for more information.

1. We used an instance of [`SKLearnProcessor`](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html#scikit-learn-processor) to run the script that transforms and splits the data. While this processor is convenient, it doesn't allow us to install additional libraries in the container. Modify the code to use an instance of [`FrameworkProcessor`](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.FrameworkProcessor) instead `SKLearnProcessor`. This class will allow you to specify a directory containing a `requirements.txt` file listing any additional dependencies. SageMaker will install these libraries in the processing container before triggering the processing job.


### Chapter 4 - Training the Model

1. The training script trains the model using a hard-coded learning rate value. Modify the script to accept the learning rate from outside the training script.

1. We currently define the number of epochs to train the model as a constant that we pass to the Estimator using the list of hyperparameters. Replace this constant with a new [Pipeline Parameter](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-parameters.html) named `training_epochs`.

1. We configured the Training Step to log information from the Training Job as part of the SageMaker Experiment associated to the pipeline. Check [Manage Machine Learning with Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) and explore the generated experiments in the SageMaker Studio Console so you can become familiar with the information SageMaker logs during training.

1. The current Training Step downloads the data from the specified S3 location and uploads it to the training container. Modify the Training Step to use SageMaker's `Pipe` input mode to stream the data directly from S3 to the training container. This will reduce the time it takes to start the Training Job and reduce the amount of disk space needed in the training container.

1. Pipeline steps can encounter exceptions. In some cases, retrying can resolve these issues. Configure the Training Step so it automatically retries the step a maximum of 5 times if it encounters an `InternalServerError`. Check the [Retry Policy for Pipeline Steps](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-retry-policy.html) documentation for more information.


### Chapter 5 - Tuning the Model

1. We are using a Tuning Step with the default strategy to select the best hyperparameters. This default strategy is `Bayesian` optimization. Modify the code to use a `Grid` strategy instead.

1. The current tuning process aims to find the model with the highest validation accuracy. Modify the code so the best model is the one with the lowest training loss.

1. Modify the the Tuning Step to find the best value of learning rate to build the model. You'll need to modify the list of hyperparameter ranges to include a `learning_rate` hyperparameter. Check the [ContiguousParameter](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html#sagemaker.tuner.ContinuousParameter) class for more information.

1. Modify the Tuning Step to include an early stopping mechanism that stops the hyperparameter search if the validation accuracy doesn't improve after a certain number of iterations.

1. Execute the pipeline to run the hyperparameter tuning jobs, and compare the results using the [Comet](https://www.comet.com/site/?utm_source=svpino_course&utm_medium=partner&utm_content=github) dashboard. You can select multiple experiments in [Comet](https://www.comet.com/site/?utm_source=svpino_course&utm_medium=partner&utm_content=github) and compare them side by side.


### Chapter 6 - Evaluating the Model

1. The evaluation script computes the accuracy of the model and exports it as part of the evaluation report. Extend the evaluation report by adding the precision and the recall of the model on each one of the classes.

1. Extend the evaluation script to test the model on each island separately. The evaluation report should contain the accuracy of the model on each island and the overall accuracy.

1. The current pipeline uses either a Training Step or a Tuning Step to build a model. Modify the pipeline to use both steps at the same time. The evaluation script should evaluate the model coming from the Training Step and the best model coming from the Tuning Step and output the accuracy and location in S3 of the best model.

### Chapter 7 - Registering the Model

1. TBD

### Chapter 8 - Conditionally Registering the Model

1. The Condition Step uses a hard-coded threshold value to determine if the model's accuracy is good enough to proceed. Modify the code so the pipeline uses the accuracy of the latest registered model version as the threshold. We want to register a new model version only if its performance is better than the previous version we registered.


### Chapter 9 - Deploying the Model Manually

1. Every SageMaker endpoint has an invocation URL you can use to generate predictions with the model from outside AWS. Write a simple Python script that runs on your local computer and sends a few samples to the endpoint. You will need your AWS access key and secret to connect to the endpoint.

### Chapter 10 - Deploying the Model Automatically

1. We can use model variants to perform A/B testing between a new model and an old model. Create a function that given the ARN of two models in the Model Registry deploys them to an endpoint as separate variants. Each variant should receive 50% of the traffic. Write another function that invokes the endpoint by default, but allows the caller to invoke a specific variant if they want to.

1. We can use SageMaker Model Shadow Deployments to create shadow variants to validate a new model version before promoting it to production. Write a function that given the ARN of a model in the Model Registry, updates an endpoint and deploys the model as a shadow variant. Check [Shadow variants](https://docs.aws.amazon.com/sagemaker/latest/dg/model-shadow-deployment.html) for more information about this topic. Send some traffic to the Endpoint and compare the results from the main model with its shadow variant.

1. SageMaker supports auto scaling models. Auto scaling dynamically adjusts the number of instances provisioned for a model in response to changes in the workload. Define a target-tracking scaling policy for a variant of your endpoint and use the `SageMakerVariantInvocationsPerInstance` metric. `SageMakerVariantInvocationsPerInstance` is the average number of times per minute that the variant is invoked. Check [Automatically Scale Amazon SageMaker Models](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html) for more information about auto scaling models.

### Chapter 11 - Deploying the Model When Approved

1. TBD

### Chapter 12 - Building an Inference Pipeline

1. TBD

### Additional SageMaker Capabilities

1. Familiarize yourself with the [Amazon SageMaker Ground Truth](https://aws.amazon.com/sagemaker/data-labeling/) service and set up a simple "Text Classification (Multi-label)" labeling job. 

