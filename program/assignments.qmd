---
title: "Assignments"
---

TBD

### Chapter 1 - Introduction and Initial Setup

1. Run the pipeline on your environment using Local Mode (`LOCAL_MODE = True`) and then switch it to run it in SageMaker (`LOCAL_MODE = False`). After completing this assignment, your environment should be fully configured, and your pipeline should run without issues.


### Chapter 2 - Exploratory Data Analysis

1. Write code to identify any potential outliers in the dataset. An outlier is a data point that differs significantly from other observations.

1. Write code to determine whether the `sex` column has any predictive power for the `species` column.

1. Write code to visualize or quantity the predictive strength of every feature in the dataset.

1. Use the [Cleanlab](https://github.com/cleanlab/cleanlab) library to find any label errors in the dataset. [Cleanlab](https://github.com/cleanlab/cleanlab) will automatically detect any issues in the dataset.

1. Use [Amazon SageMaker Data Wrangler](https://aws.amazon.com/sagemaker/data-wrangler/) to split and transform the penguin's dataset. The goal of this assignment is for you to learn how to use a no-code tool to build the preprocessing workflow.


### Chapter 3 - Splitting and Transforming the Data

1. Modify the preprocessing script to split the dataset using stratified sampling instead of random sampling.

1. The Scikit-Learn transformation pipeline automatically excludes the `sex` column from the dataset. Modify the preprocessing script so the `sex` column remains in the dataset and it's used to train the model.

1. Use ChatGPT to generate a dataset with 500 random penguins and store the file in S3. Run the pipeline pointing the `dataset_location` parameter to the new dataset. By [overriding default parameters during a pipeline execution](https://docs.aws.amazon.com/sagemaker/latest/dg/run-pipeline.html#run-pipeline-parametrized), you can process different datasets without having to modify your code.

1. We want to run a distributed Processing Job across multiple instances. This is helpful when we want to process large amounts of data in parallel. Set up a Processing Step using two instances. When specifying the input to the Processing Step, you must set the `ProcessingInput.s3_data_distribution_type` attribute to `ShardedByS3Key`. By doing this, SageMaker will run a cluster with several instances running simultaneously and distribute the input files accordingly. For this setup to work, you must have more than one input file stored in S3. Check the [`S3DataDistributionType`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_S3DataSource.html) documentation for more information.

1. Pipeline steps can encounter exceptions. In some cases, retrying can resolve these issues. Configure the Processing Step so it automatically retries the step a maximum of 3 times if it encounters an `InternalServerError`. Check the [Retry Policy for Pipeline Steps](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-retry-policy.html) documentation for more information.


### Chapter 4 - Training the Model

1. The training script trains the model using a hard-coded learning rate value. Modify the script to accept the learning rate from outside the training script.

1. Modify the training script to implement a 5-fold cross-validation strategy. You'll need to modify the pipeline so the training script receives the entire dataset instead of separate training and validation sets. 

1. We currently define the number of epochs to train the model as a constant that we pass to the Estimator using the list of hyperparameters. Replace this constant with a new [Pipeline Parameter](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-parameters.html) named `training_epochs`.

1. We configured the Training Step to log information from the Training Job as part of the SageMaker Experiment associated to the pipeline. Check [Manage Machine Learning with Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) and explore the generated experiments in the SageMaker Studio Console so you can become familiar with the information SageMaker logs during training.

1. The current Training Step downloads the data from the specified S3 location and uploads it to the training container. Modify the Training Step to use SageMaker's `Pipe` input mode to stream the data directly from S3 to the training container. This will reduce the time it takes to start the Training Job and reduce the amount of disk space needed in the training container.


### Chapter 5 - Tuning the Model

1. We are using a Tuning Step with the default strategy to select the best hyperparameters. This default strategy is `Bayesian` optimization. Modify the code to use a `Grid` strategy instead.

1. The current tuning process aims to find the model with the highest validation accuracy. Modify the code so the best model is the one with the lowest training loss.

1. Modify the the Tuning Step to find the best value of learning rate to build the model. You'll need to modify the list of hyperparameter ranges to include a `learning_rate` hyperparameter. Check the [ContiguousParameter](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html#sagemaker.tuner.ContinuousParameter) class for more information.

1. Modify the Tuning Step to include an early stopping mechanism that stops the hyperparameter search if the validation accuracy doesn't improve after a certain number of iterations.

1. Execute the pipeline to run the hyperparameter tuning jobs, and compare the results using the [Comet](https://www.comet.com/site/?utm_source=svpino_course&utm_medium=partner&utm_content=github) dashboard. You can select multiple experiments in [Comet](https://www.comet.com/site/?utm_source=svpino_course&utm_medium=partner&utm_content=github) and compare them side by side.


### Chapter 6 - Evaluating the Model

1. The evaluation script computes the accuracy of the model and exports it as part of the evaluation report. Extend the evaluation report by adding the average precision and recall of the model.

1. Extend the evaluation script to test the model on each island separately. The evaluation report should contain the accuracy of the model on each island in addition to the overall accuracy.

1. Modify the evaluation script to perform basic error analysis, identifying instances where the model predictions were incorrect. Create a new report that lists these incorrect predictions along with their true labels.

1. The current pipeline uses either a Training Step or a Tuning Step to build a model. Modify the pipeline to use both steps at the same time. The evaluation script should evaluate the model coming from the Training Step and the best model coming from the Tuning Step and output the accuracy and location in S3 of the best model.

1. Modify the evaluation script to create a [Comet](https://www.comet.com/site/?utm_source=svpino_course&utm_medium=partner&utm_content=github) experiment to track the metrics and the confusion matrix computed on the test data. You'll need to include a `requirements.txt` file with the `comet_ml` library as part of the Processing Step. SageMaker will use this file to install the library in the processing container before starting the processing job.


### Chapter 7 - Registering the Model

1. Modify the model registration process to register the model using `PendingManualApproval` as its approval status. Ensure you can approve the model manually from the Model Registry.

1. Write a function that uses the boto3's [update_model_package()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/update_model_package.html) function to appove a model registered as `PendingManualApproval`.

1. Write a function that uses [boto3's SageMaker client](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html) to get the accuracy of the latest approved model in the Model Registry. 

1. Using the boto3's [update_model_package()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/update_model_package.html) function, modify the registered model to annotate it with custom metadata. You can use this method to add additional information to the model that will enhance its discoverability and documentation. 

1. Modify the pipeline to register the model with status `Approved` only if the accuracy is above the threshold. If the accuracy is below the threshold, register the model with status `PendingManualApproval`.


### Chapter 8 - Conditional Registration

1. The existing Condition Step checks the accuracy of the model. Extend the code to register the model if its accuracy, precision, and recall are above predefined thresholds. You'll need to compute the recall and precision of the model as part of the Evaluation Step and create two more pipeline properties to specify the thresholds.

1. A common scenario is to send a notification to the team if the evaluation of the model is not successful. We can accomplish this by modifying the pipeline to include a [Callback Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-callback) to send a message to an SQS queue whenever the model's accuracy is not above the predefined threshold. A Lambda function listening to the queue can then send a notification to the team.

1. The Condition Step uses a hard-coded threshold value to determine if the model's accuracy is good enough to proceed. Modify the code so the pipeline uses the accuracy of the latest registered model version as the threshold. We want to register a new model version only if its performance is better than the previous version we registered.

1. SageMaker's [Inference Recommender](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html) can automatically help you select the best instance type and configuration (such as instance count, container parameters, and model optimizations) or serverless configuration (such as max concurrency and memory size) for your model and workload. Configure Inference Recommender for your model to get recommendations to deploy the model.

1. We logged the model's assets in [Comet](https://www.comet.com/site/?utm_source=svpino_course&utm_medium=partner&utm_content=github) as part of the training script. We want to create a new step to register the model in Comet's Model Registry. We can [register the model using Comet's SDK](https://www.comet.com/docs/v2/guides/model-management/using-model-registry/#register-a-model-from-the-sdk) from a [Lambda Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-lambda). We can reference the experiment containing the model assets using the `PIPELINE_EXECUTION_ID` execution variable.


### Chapter 9 - Serving the Model

1. TBD: Use FastAPI to do the same.
1. TBD: Use TensorFlow Serving to do the same.
1. TBD: Modify the interface to expect a JSON object instead of processed data. How can we use the transformation pipeline here?
1. TBD: Modify the script with a new function to realod the model in memory. this is helpful to upgrade the model without stopping the flask app
1. Perform post-training quantization of the model before running the Flask application. The goal is to use TensorFlow Lite to run a smaller, optimized version of the model. Check [Post-training dynamic range quantization](https://www.tensorflow.org/lite/performance/post_training_quant) for an example on how to quantize the model.


### Chapter 10 - Deploying the Model 

1. Every SageMaker endpoint has an invocation URL you can use to generate predictions with the model from outside AWS. Write a simple Python script that runs on your local computer and sends a few samples to the endpoint. You will need your AWS access key and secret to connect to the endpoint.

1. IDEA: Expose the endpoint using an API Gateway and Lambda function. This way, you can create a simple REST API to generate predictions with the model.




### Chapter 11 - Deploying From the Pipeline

1. We can use model variants to perform A/B testing between a new model and an old model. Create a function that given the ARN of two models in the Model Registry deploys them to an endpoint as separate variants. Each variant should receive 50% of the traffic. Write another function that invokes the endpoint by default, but allows the caller to invoke a specific variant if they want to.

1. We can use SageMaker Model Shadow Deployments to create shadow variants to validate a new model version before promoting it to production. Write a function that given the ARN of a model in the Model Registry, updates an endpoint and deploys the model as a shadow variant. Check [Shadow variants](https://docs.aws.amazon.com/sagemaker/latest/dg/model-shadow-deployment.html) for more information about this topic. Send some traffic to the Endpoint and compare the results from the main model with its shadow variant.

1. SageMaker supports auto scaling models. Auto scaling dynamically adjusts the number of instances provisioned for a model in response to changes in the workload. Define a target-tracking scaling policy for a variant of your endpoint and use the `SageMakerVariantInvocationsPerInstance` metric. `SageMakerVariantInvocationsPerInstance` is the average number of times per minute that the variant is invoked. Check [Automatically Scale Amazon SageMaker Models](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html) for more information about auto scaling models.



### Chapter 12 - Deploying From an Event

1. TBD



### Chapter 13 - Building an Inference Pipeline

1. Using the boto3's [`describe_model_package()`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/describe_model_package.html) function, compare the `InferenceSpecification` of the pipeline model with the `InferenceSpecification` of the basic model we created before. Notice how the pipeline model lists the three different containers we used to build the inference pipeline.



### Chapter 14 - Data Quality Baseline

1. TBD



### Additional SageMaker Capabilities

1. Familiarize yourself with the [Amazon SageMaker Ground Truth](https://aws.amazon.com/sagemaker/data-labeling/) service and set up a simple "Text Classification (Multi-label)" labeling job. 


IDEAS:

- Create a chapter to deploy custom Docker container
- Redesign AWS diagrams
- Finish setup instructions
- Whenever I build the inference pipeline I should compare the containers (3) from the model with the previous model (1 container)

