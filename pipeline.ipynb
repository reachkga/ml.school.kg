{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9992fc69-4868-4e50-a50e-76da237f70ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SakeMaker Pipelines\n",
    "\n",
    "This notebook uses the [penguins dataset](https://www.kaggle.com/parulpandey/palmer-archipelago-antarctica-penguin-data).\n",
    "\n",
    "<img src='https://imgur.com/orZWHly.png' alt='Penguins dataset' width=\"1024\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1e3a2-dbca-4cae-8cd3-f00c7203f967",
   "metadata": {},
   "source": [
    "Let's make sure we are running the latest version of the SakeMaker's SDK. Restart the notebook after you upgrade the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72eb4984-3c21-4033-be45-2de2cfc1257c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.22.22 requires botocore==1.23.22, but you have botocore 1.29.109 which is incompatible.\n",
      "awscli 1.22.22 requires s3transfer<0.6.0,>=0.5.0, but you have s3transfer 0.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mName: sagemaker\n",
      "Version: 2.145.0\n",
      "Summary: Open source library for training and deploying models on Amazon SageMaker.\n",
      "Home-page: https://github.com/aws/sagemaker-python-sdk/\n",
      "Author: Amazon Web Services\n",
      "Author-email: \n",
      "License: Apache License 2.0\n",
      "Location: /usr/local/lib/python3.8/site-packages\n",
      "Requires: attrs, boto3, google-pasta, importlib-metadata, jsonschema, numpy, packaging, pandas, pathos, platformdirs, protobuf, protobuf3-to-dict, PyYAML, schema, smdebug-rulesconfig\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade pip\n",
    "!pip install -q --upgrade sagemaker\n",
    "!pip show sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38154d8f-c6c0-48f9-a20d-a4232b31e953",
   "metadata": {},
   "source": [
    "Look at:\n",
    "\n",
    "* Caching Pipeline Steps https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-caching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "915b1d0b-d9da-4529-aca5-fd1c08a36f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString, ParameterFloat\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import ModelPackage\n",
    "\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accefa0d-31d6-4df4-814a-d74c703689b5",
   "metadata": {},
   "source": [
    "## Step 1 - Downloading the Dataset\n",
    "\n",
    "The first step is to download the dataset and store it in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882a8be9-dc31-4df1-9009-3b2d0284cb70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://mlschool/penguins/data.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FILEPATH = \"penguins/data.csv\"\n",
    "S3_FILEPATH = \"s3://mlschool/penguins\"\n",
    "\n",
    "# Download the official Penguins dataset and store it locally.\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins_size.csv\", \n",
    "    DATA_FILEPATH\n",
    ")\n",
    "\n",
    "# Upload the dataset to S3. We need to do this to make it available to \n",
    "# the preprocessing step.\n",
    "INPUT_DATA_URI = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=DATA_FILEPATH, \n",
    "    desired_s3_uri=S3_FILEPATH,\n",
    ")\n",
    "\n",
    "INPUT_DATA_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3938a1d-d5bd-4967-ad65-a945d5df1f61",
   "metadata": {},
   "source": [
    "## Step 2 - Configuring the Pipeline\n",
    "\n",
    "These are the parameters that will be used by the Pipeline:\n",
    "\n",
    "* `dataset_location`: This parameter represents the location of the dataset in S3. We'll use this parameter during the Preprocessing Step to perform feature engineering on the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86de7edd-18b0-40d1-ac8e-0f3ef4469be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_location = ParameterString(\n",
    "    name=\"dataset_location\",\n",
    "    default_value=INPUT_DATA_URI,\n",
    ")\n",
    "\n",
    "# We need to predefine the location where the preprocessing step will be\n",
    "# storing the dataset splits to avoid SageMaker from appending a timestamp\n",
    "# to their auto-generated location. If we let SageMaker use a timestamp, \n",
    "# we can't cache this step.\n",
    "preprocessor_destination = ParameterString(\n",
    "    name=\"preprocessor_destination\",\n",
    "    default_value=f'{S3_FILEPATH}/preprocessing',\n",
    ")\n",
    "\n",
    "\n",
    "# This represents the approval status of a new trained model in the registry.\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"model_approval_status\", \n",
    "    default_value=\"Approved\"\n",
    ")\n",
    "\n",
    "# This property represents the minimum accuracy that the model should\n",
    "# reach in order for it to be registered.\n",
    "accuracy_threshold = ParameterFloat(\n",
    "    name=\"accuracy_threshold\", \n",
    "    default_value=0.75\n",
    ")\n",
    "\n",
    "\n",
    "# We can use this cache configuration to cache individual pipeline steps.\n",
    "# If we enable it, SageMaker will reuse the output of a previous successful\n",
    "# run.\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"5d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9771e08-5560-492a-845e-f06988b6ae1b",
   "metadata": {},
   "source": [
    "## Step 3 - Preprocessing the Dataset\n",
    "\n",
    "This script is responsible from doing feature engineering on the original dataset and spliting the data into train, validation, and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb6ba7c0-1bd6-4fe5-8b7f-f6cbdfd3846c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguins/preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile penguins/preprocessor.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "BASE_DIR = \"/opt/ml/processing\"\n",
    "INPUT_PATH = Path(BASE_DIR) / \"input\"\n",
    "DATA_FILEPATH = INPUT_PATH / \"data.csv\"\n",
    "\n",
    "\n",
    "def preprocess(base_dir, data_filepath):\n",
    "    df = pd.read_csv(data_filepath)\n",
    "    \n",
    "    numerical_columns = [column for column in df.columns if df[column].dtype in [\"int64\", \"float64\"]]\n",
    "\n",
    "    numerical_preprocessor = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_preprocessor = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"numerical\", numerical_preprocessor, numerical_columns),\n",
    "            (\"categorical\", categorical_preprocessor, [\"island\"]),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "\n",
    "    X = df.drop([\"sex\"], axis=1)\n",
    "    columns = list(X.columns)\n",
    "    \n",
    "    X = X.to_numpy()\n",
    "    \n",
    "    np.random.shuffle(X)\n",
    "    train, validation, test = np.split(X, [int(.7 * len(X)), int(.85 * len(X))])\n",
    "    \n",
    "    X_train = pd.DataFrame(train, columns=columns)\n",
    "    X_validation = pd.DataFrame(validation, columns=columns)\n",
    "    X_test = pd.DataFrame(test, columns=columns)\n",
    "    \n",
    "    y_train = X_train.species\n",
    "    y_validation = X_validation.species\n",
    "    y_test = X_test.species\n",
    "\n",
    "    X_train.drop([\"species\"], axis=1, inplace=True)\n",
    "    X_validation.drop([\"species\"], axis=1, inplace=True)\n",
    "    X_test.drop([\"species\"], axis=1, inplace=True)\n",
    "    \n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_validation = preprocessor.transform(X_validation)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_validation = label_encoder.transform(y_validation)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "    \n",
    "    \n",
    "    train = np.concatenate((X_train, np.expand_dims(y_train, axis=1)), axis=1)\n",
    "    validation = np.concatenate((X_validation, np.expand_dims(y_validation, axis=1)), axis=1)\n",
    "    test = np.concatenate((X_test, np.expand_dims(y_test, axis=1)), axis=1)\n",
    "    \n",
    "    train_path = Path(base_dir) / \"train\" \n",
    "    validation_path = Path(base_dir) / \"validation\" \n",
    "    test_path = Path(base_dir) / \"test\"\n",
    "    \n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    validation_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(validation_path / \"validation.csv\", header=False, index=False)\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run(BASE_DIR, DATA_FILEPATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e95aa06-6527-4126-988f-d243b8829c6f",
   "metadata": {},
   "source": [
    "We can now create a Processing Step to run our preprocessing script. We will use a Scikit-Learn processor.\n",
    "\n",
    "The input of this step will be the dataset location, and the output will be the location of the three sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b982bc3-28f0-473f-b8d2-fc35dd251e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    base_job_name=\"penguins-preprocessing-processor\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "preprocess_step = ProcessingStep(\n",
    "    name=\"PenguinsPreprocessor\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=dataset_location, destination=\"/opt/ml/processing/input\"),  \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\", destination=preprocessor_destination),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\", destination=preprocessor_destination),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\", destination=preprocessor_destination),\n",
    "    ],\n",
    "    code=\"penguins/preprocessor.py\",\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d7ab91-12ed-45ee-adde-0c4f435ac0f6",
   "metadata": {},
   "source": [
    "## Step 4 - Training the Model\n",
    "\n",
    "This script is responsible from training a simple neural network on the train data, validating the model, and saving it so we can later use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee421a1d-5d13-4cce-ab89-f6570b27ab25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguins/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile penguins/train.py\n",
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Conv2D, Dense, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dropout\n",
    "\n",
    "\n",
    "def train(base_directory, train_path, validation_path, epochs=50, batch_size=32):\n",
    "    X_train = pd.read_csv(Path(train_path) / \"train.csv\")\n",
    "    y_train = X_train[X_train.columns[-1]]\n",
    "    X_train.drop(X_train.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "    X_validation = pd.read_csv(Path(validation_path) / \"validation.csv\")\n",
    "    y_validation = X_validation[X_validation.columns[-1]]\n",
    "    X_validation.drop(X_validation.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(10, input_shape=(X_train.shape[1],), activation=\"relu\"),\n",
    "        Dense(8, activation=\"relu\"),\n",
    "        Dense(3, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=SGD(learning_rate=0.01),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        validation_data=(X_validation, y_validation),\n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    predictions = np.argmax(model.predict(X_validation), axis=-1)\n",
    "    print(f\"Validation accuracy: {accuracy_score(y_validation, predictions)}\")\n",
    "    \n",
    "    model_filepath = Path(base_directory) / \"model\" / \"001\"\n",
    "    model.save(model_filepath)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--base_directory\", type=str, default=\"/opt/ml/\")\n",
    "    parser.add_argument(\"--train_path\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\", None))\n",
    "    parser.add_argument(\"--validation_path\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\", None))\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    train(\n",
    "        base_directory=args.base_directory,\n",
    "        train_path=args.train_path,\n",
    "        validation_path=args.validation_path,\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3fe6b-0451-4e7e-aac4-88c5e2db52cb",
   "metadata": {},
   "source": [
    "The Training Step will use a TensorFlow estimator.\n",
    "\n",
    "The inputs to this step are the location of the train and the validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e0404c7-dba5-4e9c-9c90-d787232ab4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 32\n",
    "}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point=\"penguins/train.py\",\n",
    "    role=role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    py_version=\"py37\",\n",
    "    framework_version=\"2.4\",\n",
    "    script_mode=True,\n",
    "    volume_size=5,\n",
    "    disable_profiler=True\n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name=\"PenguinsTraining\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=preprocess_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=preprocess_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    },\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4f9617-2957-4f7f-b30e-6e26d857907f",
   "metadata": {},
   "source": [
    "## Step 5 - Evaluating the Model\n",
    "\n",
    "This script is reponsible from loading the model and evaluating it on the test set. Before finishing, this script will create a file containing the evaluation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bdfebff-4b60-430a-939c-975660a94508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguins/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile penguins/evaluation.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "MODEL_PATH = \"/opt/ml/processing/model/\"\n",
    "TEST_PATH = \"/opt/ml/processing/test/\"\n",
    "OUTPUT_PATH = \"/opt/ml/processing/evaluation/\"\n",
    "\n",
    "\n",
    "def evaluate(model_path, test_path, output_path):\n",
    "    with tarfile.open(Path(model_path) / \"model.tar.gz\") as tar:\n",
    "        tar.extractall(path=Path(model_path))\n",
    "        \n",
    "    dir_list = os.listdir(Path(model_path))\n",
    "    print(\"model_path\", dir_list)\n",
    "    \n",
    "    model = keras.models.load_model(Path(model_path) / \"001\")\n",
    "    \n",
    "    X_test = pd.read_csv(Path(test_path) / \"test.csv\")\n",
    "    y_test = X_test[X_test.columns[-1]]\n",
    "    X_test.drop(X_test.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "    predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Test accuracy: {accuracy}\")\n",
    "    \n",
    "    evaluation_report = {\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": {\n",
    "                \"value\": accuracy\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(Path(output_path) / \"evaluation.json\", \"w\") as f:\n",
    "        f.write(json.dumps(evaluation_report))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate(\n",
    "        model_path=MODEL_PATH, \n",
    "        test_path=TEST_PATH,\n",
    "        output_path=OUTPUT_PATH\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223c6cd2-3ba7-4305-9b25-7e10bc0e8a1b",
   "metadata": {},
   "source": [
    "The Processing Step will use a Script Processor running TensorFlow.\n",
    "\n",
    "The inputs to this step are the location of the saved model and the location of the test set. The output is the evaluation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4ced58b-485d-42f5-b6c3-d95ac055cf03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=\"2.4\",\n",
    "    py_version=\"py37\",\n",
    "    image_scope=\"training\",\n",
    "    instance_type=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "evaluation_script_processor = ScriptProcessor(\n",
    "    base_job_name=\"penguins-evaluation-processor\",\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "evaluation_step = ProcessingStep(\n",
    "    name=\"PenguinsEvaluation\",\n",
    "    processor=evaluation_script_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=preprocess_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"penguins/evaluation.py\",\n",
    "    property_files=[evaluation_report],\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69015908-ef41-4427-95b1-4fa6b706ceae",
   "metadata": {},
   "source": [
    "## Step 6 - Registering the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3af4ce0f-1166-4951-aa33-12b7f374bdf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py:270: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_package_group_name = \"PenguinsModelPackage\"\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=f\"{evaluation_step.arguments['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']}/evaluation.json\",\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=\"2.4\",\n",
    "    py_version=\"py37\",\n",
    "    image_scope=\"inference\",\n",
    "    instance_type=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=PipelineSession(),\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "register_args = model.register(\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    model_metrics=model_metrics,\n",
    "    approval_status=model_approval_status,\n",
    ")\n",
    "\n",
    "register_model_step = ModelStep(\n",
    "   name=\"PenguinsCreateModel\",\n",
    "   step_args=register_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa1f8f-5d8e-49be-9b88-afc402692bfc",
   "metadata": {},
   "source": [
    "## Step 7 - Verifying the Model's Accuracy\n",
    "\n",
    "We want to register the model only if their accuracy is above a predefined threshold. We can use a Condition Step to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9461ec6-6e4e-4b9d-b840-459ee91d80cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "condition_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"metrics.accuracy.value\"\n",
    "    ),\n",
    "    right=accuracy_threshold\n",
    ")\n",
    "\n",
    "accuracy_condition_step = ConditionStep(\n",
    "    name=\"PenguinsAccuracyCondition\",\n",
    "    conditions=[condition_gte],\n",
    "    if_steps=[register_model_step],\n",
    "    else_steps=[], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1465690a-67b5-4d68-a026-dd874ee569e8",
   "metadata": {},
   "source": [
    "## Step 8 - Running the Pipeline\n",
    "\n",
    "We can now define and run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "564fe6fb-00aa-480f-a58f-3144b2e33a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=\"PenguinsPipeline\",\n",
    "    parameters=[\n",
    "        dataset_location, \n",
    "        preprocessor_destination,\n",
    "        model_approval_status,\n",
    "        accuracy_threshold,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_step, \n",
    "        training_step, \n",
    "        evaluation_step,\n",
    "        accuracy_condition_step\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4410f2f-a17b-4272-abd2-e092e134174c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n"
     ]
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1970612-eb2f-4020-971b-068fc004f052",
   "metadata": {},
   "source": [
    "## Step 9 - Deploying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ae64ffb-19b8-4b23-ab9a-f2c0c49c1f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelPackageGroupName': 'PenguinsModelPackage',\n",
       " 'ModelPackageVersion': 1,\n",
       " 'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:325223348818:model-package/penguinsmodelpackage/1',\n",
       " 'CreationTime': datetime.datetime(2023, 4, 8, 18, 48, 41, 648000, tzinfo=tzlocal()),\n",
       " 'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.4-cpu',\n",
       "    'ImageDigest': 'sha256:9655faca3ac7c26a9583ad05a324ca7a58da4af1c14f3d2daae0d217688356a9',\n",
       "    'ModelDataUrl': 's3://sagemaker-us-east-1-325223348818/pipelines-uuwrvnosz7n5-PenguinsTraining-ojUeWdBAwE/output/model.tar.gz',\n",
       "    'Environment': {}}],\n",
       "  'SupportedTransformInstanceTypes': ['ml.m5.large'],\n",
       "  'SupportedRealtimeInferenceInstanceTypes': ['ml.m5.large'],\n",
       "  'SupportedContentTypes': ['text/csv'],\n",
       "  'SupportedResponseMIMETypes': ['text/csv']},\n",
       " 'ModelPackageStatus': 'Completed',\n",
       " 'ModelPackageStatusDetails': {'ValidationStatuses': [],\n",
       "  'ImageScanStatuses': []},\n",
       " 'CertifyForMarketplace': False,\n",
       " 'ModelApprovalStatus': 'Approved',\n",
       " 'CreatedBy': {'IamIdentity': {'Arn': 'arn:aws:sts::325223348818:assumed-role/AmazonSageMaker-ExecutionRole-20230312T160501/sagemaker-pipeline-hi29vuztfd6l-PenguinsCreateModel-',\n",
       "   'PrincipalId': 'AROAUXOGSPJJGTQ2RIUND:sagemaker-pipeline-hi29vuztfd6l-PenguinsCreateModel-'}},\n",
       " 'MetadataProperties': {'GeneratedBy': 'arn:aws:sagemaker:us-east-1:325223348818:pipeline/penguinspipeline/execution/hi29vuztfd6l'},\n",
       " 'ModelMetrics': {'ModelQuality': {'Statistics': {'ContentType': 'application/json',\n",
       "    'S3Uri': 's3://sagemaker-us-east-1-325223348818/PenguinsEvaluation-c87882a1d476c3cd59f994fbde5f5f99/output/evaluation/evaluation.json'}},\n",
       "  'Bias': {},\n",
       "  'Explainability': {}},\n",
       " 'ResponseMetadata': {'RequestId': 'b743a58b-ff64-4bcc-843c-9d47dbaaee84',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b743a58b-ff64-4bcc-843c-9d47dbaaee84',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1563',\n",
       "   'date': 'Sat, 08 Apr 2023 20:36:43 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "\n",
    "def get_latest_approved_model_package(model_package_group_name):\n",
    "    \"\"\"\n",
    "    Returns the latest approved model package registered under the \n",
    "    supplied model package group.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = sagemaker_client.list_model_packages(\n",
    "            ModelPackageGroupName=model_package_group_name,\n",
    "            ModelApprovalStatus=\"Approved\",\n",
    "            SortBy=\"CreationTime\",\n",
    "            MaxResults=100,\n",
    "        )\n",
    "        approved_packages = response[\"ModelPackageSummaryList\"]\n",
    "\n",
    "        while len(approved_packages) == 0 and \"NextToken\" in response:\n",
    "            response = sagemaker_client.list_model_packages(\n",
    "                ModelPackageGroupName=model_package_group_name,\n",
    "                ModelApprovalStatus=\"Approved\",\n",
    "                SortBy=\"CreationTime\",\n",
    "                MaxResults=100,\n",
    "                NextToken=response[\"NextToken\"],\n",
    "            )\n",
    "            approved_packages.extend(response[\"ModelPackageSummaryList\"])\n",
    "\n",
    "        if len(approved_packages) == 0:\n",
    "            error = f\"No approved model pacakages for {model_package_group_name}\"\n",
    "            print(error)\n",
    "            raise Exception(error)\n",
    "\n",
    "        # Return the pmodel package arn\n",
    "        model_package_arn = approved_packages[0][\"ModelPackageArn\"]\n",
    "        logger.info(f\"Identified the latest approved model package: {model_package_arn}\")\n",
    "        return approved_packages[0]\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "        raise Exception(e.response[\"Error\"][\"Message\"])\n",
    "        \n",
    "        \n",
    "def does_endpoint_exist(endpoint_name):\n",
    "    \"\"\"\n",
    "    Returns whether the supplied endpoint already exists.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        return False\n",
    "        \n",
    "\n",
    "approved_model_package = get_latest_approved_model_package(model_package_group_name)\n",
    "approved_model_package_arn = approved_model_package[\"ModelPackageArn\"]\n",
    "\n",
    "model_description = sagemaker_client.describe_model_package(\n",
    "    ModelPackageName=approved_model_package_arn\n",
    ")\n",
    "\n",
    "model_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a54eaa4-a37b-4e97-806a-7200daffe740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package = ModelPackage(\n",
    "    role=role, \n",
    "    model_package_arn=approved_model_package_arn, \n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "endpoint_name = \"penguins-endpoint\"\n",
    "\n",
    "if not does_endpoint_exist(endpoint_name):\n",
    "    model_package.deploy(\n",
    "        initial_instance_count=1, \n",
    "        instance_type=\"ml.m5.large\", \n",
    "        endpoint_name=endpoint_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84deb8de-a703-4896-a31d-c2497b7d5703",
   "metadata": {},
   "source": [
    "## Step 10 - Testing the Model Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af05395f-b512-473f-9cd2-15b77d729b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 2\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(endpoint_name=endpoint_name)\n",
    "\n",
    "payload = \"0.6569590202313976, -1.0813829646495108, 1.2097102831892812, 0.9226343641317372, 1.0, 0.0, 0.0\"\n",
    "p = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\n",
    "\n",
    "predictions = json.loads(p.decode(\"utf-8\"))[\"predictions\"]\n",
    "\n",
    "print(f\"Prediction: {np.argmax(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032460ff-eacc-4945-ba74-8782b2d02660",
   "metadata": {},
   "source": [
    "## Step 11 - Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "83c39d12-77e1-48b5-908d-df28bf591ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:325223348818:pipeline/penguinspipeline',\n",
       " 'ResponseMetadata': {'RequestId': '533f9d2b-1aa8-4545-a30e-c98f4bd9862d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '533f9d2b-1aa8-4545-a30e-c98f4bd9862d',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '84',\n",
       "   'date': 'Sat, 08 Apr 2023 20:40:25 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model_package in sagemaker_client.list_model_packages(\n",
    "    ModelPackageGroupName=model_package_group_name)[\"ModelPackageSummaryList\"]:\n",
    "    print(f\"Deleting {model_package['ModelPackageArn']}\")\n",
    "    sagemaker_client.delete_model_package(ModelPackageName=model_package[\"ModelPackageArn\"])\n",
    "\n",
    "    \n",
    "sagemaker_client.delete_model_package_group(ModelPackageGroupName=model_package_group_name)\n",
    "predictor.delete_endpoint()\n",
    "pipeline.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4bf547-a1df-477b-a0f9-0b9c53f32655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.6-cpu-py38-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
